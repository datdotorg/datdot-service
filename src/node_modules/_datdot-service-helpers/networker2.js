const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const HypercoreProtocol = require('hypercore-protocol')
const RAM = require('random-access-memory')
const register_extension = require('_datdot-service-helpers/register-extension')
const ready = require('_datdot-service-helpers/hypercore-ready')
const datdot_crypto = require('datdot-crypto')
const FeedStorage = require('_datdot-service-helpers/feed-storage.js')
const sub = require('subleveldown')
var pump = require('pump')


module.exports = networker

function networker (account) {
    const api = {
        start_task: (opts) => _start_task(account, opts),
        end_task: (...args) => _end_task(account, ...args)
    }
    return api
}

function noop () {}

async function _start_task(account, { config, extension = {}, swarm_opts, peers, feedkey, log }) {
    const { intercept, fresh, persist } = config // feed config
    var { topic, mode } =  swarm_opts
    const { ext_cbs, name } = extension
    peers = Object.assign({ peerList: [], onpeer: noop }, peers)
    const { peerList, onpeer } = peers
    
    log({ type: 'feed-store', data: { text: 'loading feed', config, extension, swarm_opts, peers, feedkey }})
    
    var feed
    var feedkey
    var stringkey
    var swarm
    var stringtopic
    var storage
    const cache = account.cache
    
    if (feedkey) {
        topic = datdot_crypto.get_discoverykey(feedkey)
        stringkey = feedkey.toString('hex')
        stringtopic = topic.toString('hex')
    }

    // FRESH FEEDS
    if (fresh) {
        const swarmID = Object.keys(cache['fresh']).length + 1
        const subcache_path = ['fresh', swarmID]
        const subcache = load_subcache(subcache_path)


        // 1. Get or make feed 
        if (feedkey) feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
        else feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
        await ready(feed)
        if (!feedkey) {
            feedkey = feed.key
            topic = datdot_crypto.get_discoverykey(feedkey)
            stringtopic = topic.toString('hex')
            stringkey = feedkey.toString('hex')
        }
        log({ type: 'feed-store', data: { text: 'New feed - fresh', stringkey, stringtopic }})
        // 2. Store topic and feed
       subcache.topics = { [stringtopic]: { feeds: {}, sockets: {} } } // add next entry to cache
       subcache.topics[stringtopic].feeds[stringkey] = { feed }
        log({ type: 'feed-store', data: { text: 'New topic stored in cache - fresh', stringtopic }})
        // 3. Make swarm
        swarm = new hyperswarm()
       subcache.swarm = swarm
        log({ type: 'feed-store', data: { text: 'new swarm - fresh', mode }})
        swarm.on('connection', onconnection(swarmID, log))
        // 4. Join topic (if no discovery yet)
        var discovery = subcache.topics[stringtopic].discovery
        if (!discovery) subcache.topics[stringtopic].discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        increase_task_count('fresh', swarmID)
    } 
    // INTERCEPTED FEEDS
    else if (intercept) { 
        const subcache_path = ['intercept']
        const subcache = load_subcache(subcache_path)
        // 1. Get or make feed
        const topics = subcache.topics
        if (topics[stringtopic] && topics[stringtopic].feeds[stringkey]) { // get existing feed
            feed = topics[stringtopic].feeds[stringkey].feed 
            log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
        } else { // make new feed
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            await ready(feed)
            log({ type: 'feed-store', data: { text: 'New feed', intercept, stringkey, stringtopic }})
        }
        // 2. Store topic and feed
        if (!topics[stringtopic]) topics[stringtopic] = { feeds: {}, sockets: {} }
        if (!topics[stringtopic].feeds[stringkey]) { // stringtopic exists, let's add the stringkey to the feeds
            const db = sub(account.db, stringkey, { valueEncoding: 'binary' })
            const storage = new FeedStorage({ db, feed, log })
            topics[stringtopic].feeds[stringkey] = { feed, storage }
            log({ type: 'feed-store', data: { text: 'New feed and storage added in cache', intercept, stringkey, stringtopic }})
        }
        // 3. Make swarm
        if (!subcache.swarm) {
            const opts = { keyPair: { publicKey: account.noisePublicKey, secretKey: account.noisePrivateKey } }
            swarm = new hyperswarm(opts)
            subcache.swarm = swarm
            log({ type: 'feed-store', data: { text: 'new swarm - intercept', mode }})
            swarm.on('connection', onconnection(swarmID, log))
        } else swarm = subcache.swarm
        // 4. Join topic (if no discovery yet)
        var discovery = subcache.topics[stringtopic].discovery
        if (!discovery) subcache.topics[stringtopic].discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        increase_task_count(swarmID)
        // 6. Add ext (if name & no ext yet)
        var ext = topics[stringtopic].ext
        if (!ext) topics[stringtopic].ext = register_extension(feed, name, ext_cbs.onmessage, ext_cbs.onerror)
    } 
    // REUSABLE UNINTERCEPTED FEEDS
    else { // GENERAL:
        const subcache_path = ['general']
        const subcache = load_subcache(subcache_path)
        const topics = subcache.topics
        // 1. Get or make feed
        if (topics[stringtopic] && topics[stringtopic].feeds[stringkey]) { // get existing feed
            feed = topics[stringtopic].feeds[stringkey].feed 
            log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
        } else { // make new feed
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            await ready(feed)
            log({ type: 'feed-store', data: { text: 'New feed', stringkey, stringtopic }})
        }
        // 2. Store topic and feed
        if (!topics[stringtopic]) topics[stringtopic] = { feeds: {}, sockets: {} }
        if (!topics[stringtopic].feeds[stringkey]) topics[stringtopic].feeds[stringkey] = { feed }
        // 3. Make swarm
        if (!subcache.swarm) {
            swarm = new hyperswarm()
            subcache.swarm = swarm
            log({ type: 'feed-store', data: { text: 'new swarm - general', mode }})
            swarm.on('connection', onconnection(swarmID, log))
        } else swarm = subcache.swarm
        // 4. Join topic (if no discovery yet)
        var discovery = subcache.topics[stringtopic].discovery
        if (!discovery) subcache.topics[stringtopic].discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        increase_task_count(swarmID)
    }
 
    log({ type: 'feed-store', data: { text: 'returning feed' }})
    return { feed, swarmID }

    function onconnection (swarmID, log) {

        var cache_type
        var replicationStream

        if (fresh) cache_type = cache['fresh'][swarmID]
        else if (intercept) cache_type = cache['intercept']
        else cache_type = cache['general']
        const { swarm, sockets, topics } = cache_type 

        /*
        max amount of sockets peer can have open (i.e. const maxsockets = 64)
        - new socket
            - if (socket_count >= maxsockets) connection.disconnect() // HOW?
            - tasks: if:
                - encoding
                    - if socket.topic matches active task => connect
                - attestor - perf challenge
                    - if socket.remoteKey matches active task (peer) => connect
                - hosting
                    - client phase:
                        - if socket.topic matches active task => connect
                    - server phase:
                        - connect
                    -
                - sponsor
                    - if socket.topic matches active task => connect
        */

        return (connection, info) => {
            // .. 
            const swarmcache = get_swarm_cache(swarmID)

        }

        return (connection, info) => {
            const remotekey = connection.remotePublicKey
            const remotestringkey = remotekey.toString('hex')
            if (sockets[remotestringkey]) {
                log({ type: 'load-feed', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = sockets[remotestringkey].replicationStream
            } else {
                log({ type: 'load-feed', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = new HypercoreProtocol(connection.isInitiator)
                sockets[remotestringkey] = { socket: connection, replicationStream }
                // connection.pipe(replicationStream).pipe(connection)
                pump(connection, replicationStream, connection, (error) => {
                    log({ type: 'load-feed', data: { text: 'Pumping finished' }})
                    if (error) console.log({ name: log.path, pumping_error: error})
                })
            }
            
            if (peerList.length) return is_on_peerList()
            
            // log({ type: 'swarm', data: { text: `New connection`, socket: socket.isInitiator  } })
            // next({ ext, feed, remotekey, log })

            /*
            1. hoster, downloading data from author
            2. feed is stored in cache and storage
            3. 
            */
            if (info.topics.length) {
                for (var i = 0, len = info.topics.length; i < len; i++) {
                    const Topic = info.topics[i].toString('hex')
                    log({ type: 'load-feed', data: { text: 'topics', Topic, isInitiator: connection.isInitiator } })
                    if (topics[Topic]) {
                        var topics_sockets = topics[Topic].sockets
                        if (!topics_sockets[[remotestringkey]]) topics_sockets[remotestringkey] = sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - client', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                }
            } else {
                replicationStream.on('discovery-key', (discoverykey) => { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
                    log({ type: 'onconnection', data: { text: 'discovery key' } })
                    const Topic = discoverykey.toString('hex')
                    if (topics[Topic]) { 
                        var topics_sockets = topics[Topic].sockets
                        if (!topics_sockets[[remotestringkey]]) topics_sockets[remotestringkey] = sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - server', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                })
            }
            function is_on_peerList () {
                var peerList_temp = [...peerList]
                log({ type: 'feed-store', data: { text: `is on peer list`, peerList  } })
                for (var i = 0, len = peerList_temp.length; i < len; i++) {
                    const hosterkey = peerList_temp[i]
                    log({ type: 'attestor challenge', data: { text: `Comparing keys`, peer: remotekey.toString('hex'), hoster: hosterkey.toString('hex') } })
                    if (remotekey.equals(hosterkey)) {
                        log({ type: 'feed-store', data: { text: `Found the right hoster`, id: hosterkey.toString('hex'), feed: feed.key.toString('hex')  } })
                        peerList_temp.splice(i, 1)
    
                        feed.replicate(replicationStream)
                        return onpeer(hosterkey)
                    }
                }
            }

            replicationStream.on('close', (err) => { log({ type: 'feed-store', data: { text: `replicationStream closed` } }) })
            replicationStream.on('end', (err) => { log({ type: 'feed-store', data: { text: `replicationStream ended` } }) })
            replicationStream.on('timeout', (err) => { log({ type: 'feed-store', data: { text: `replicationStream timed out` } }) })
        }
    }
}

async function _end_task (account, args) {
    const { stringtopic, log } = args
    delete account.cache.tasks[stringtopic]
    const remotekeys = Object.keys(account.cache.topics[stringtopic].sockets)
    // close all the streams
    remotekeys.forEach(key => {
        const { socket, replicationStream } = account.cache.sockets[key]
        // // if socket not in use, close it
        // socket.close()
        // replicationStream.close()
        // // delete the socket object
        // delete account.cache.sockets[key]
    })
    // close all the feeds
    const feedkeys = Object.keys(account.cache.topics[stringtopic].feeds)
    feedkeys.forEach(async key => {
        const feed = account.cache.topics[stringtopic].feeds[key]
        await feed.close()
    })
    // destroy the discovery
    // account.cache.topics[stringtopic].discovery.leave()
    await account.cache.topics[stringtopic].discovery.destroy()
    // delete the topic object
    delete account.cache.topics[stringtopic]
}

function get_swarm_cache (...keys) {
    for (var i = 0, temp = cache, len = keys.length; i < len; i++) temp = temp[keys[i]]
    return temp
}
function increase_task_count (...keys) {
    const temp = get_swarm_cache(...keys)
    temp.topics[stringtopic].feeds[stringkey].task_count++
    temp.topics[stringtopic].task_count++
    temp.task_count++
}
function load_subcache (keys) {
    const key = keys.pop()
    for (var i = 0, temp = cache, len = keys.length; i < len; i++) temp = temp[keys[i]]
    if (!temp[key]) temp[key] = { sockets: {}, topics: {}, task_count: 0 }
    return temp[key]
}


////////////////////////////////////////



////////////////////////////////////////

/* 
account = {
    cache:  {
        swarm,
        sockets: {
            remotekey: { 
                socket, 
                replicationStream
            }
        },
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { feed }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        },
        tasks: {
            stringkey: counter
        }
    },
    storage: {
        feedkey: { feed, db, discovery, unintercept }
    }
} 







 SCENARIOS

 cache: {
     'general': {
        swarm, // autogenerated keypair
        sockets,
        topics,
        task_count
     },
     'fresh': {
         '1': {
            swarm, // @TODO reuse swarm for feeds with different topic
            sockets,
            topics,
            task_count    
        }
    },
     'intercept': {
        swarm, // noise keypair, stored on chain
        sockets,
        topics: {
            stringtopic1: {
                discovery,
                feeds: {
                    stringkey1: { 
                        feed,
                        storage,
                        task_count: 1
                    },
                    stringkey2: { 
                        feed,
                        storage,
                        task_count: 2
                    },
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                },
                task_count: 3
            },
            stringtopic2: {
                task_count: 3
            }
        },
        task_count: 6
     }
 }

 ----
 ENCODING

 - starting

* feed !fresh
* can reuse a feed from other !fresh (sponsor)
* if no feed yet, make new
* if no swarm, create one (store it in cache under swarm pubkey)

 - finishing

 * close swarm 
 * leave topic/destroy discovery
 * close feed
 * close socket/replication streams
    * a socket that connects and doesnt aks for data soon after should be disconnected to make space for other sockets, unless hyperswarm does that already somehow internally
    * 

 ----
 ATTESTING - performance challenge

  - starting

 * feed is fresh

 - finishing


 ----
 HOSTING

 - starting

 * feed is intercepted && !fresh


 - finishing

 ----
 AUTHOR

  - starting

* feed is fresh


 - finishing


 ----
 SPONSOR

 - starting

 * feed !fresh

 - finishing

---

*/

