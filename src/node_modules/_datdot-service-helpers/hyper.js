const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const RAM = require('random-access-memory')
const datdot_crypto = require('datdot-crypto')
const mux_proto = require('_datdot-service-helpers/mux-proto')
const b4a = require('b4a')
const c = require('compact-encoding')
const try_refresh_discovery = require('_datdot-service-helpers/try-refresh-discovery')
const safetyCatch = require('safety-catch')
const DEFAULT_TIMEOUT = 5000
const {
	done_task_cleanup,
	clean_general_task,
	get_tasks_count_for_topic,
	close_feeds,
	remove_from_tasks,
	get_connections,
	remove_from_feeds,
	decrease_socket_count,
	remove_from_targets,
	close_streams_and_channels,
	get_target_tasks,
	remove_from_target_tasks,
	remove_from_task_connections,
	remove_from_registry,
	get_tasks,
	set_status_for_connection
} = require('_datdot-service-helpers/done-task-cleanup')


module.exports = hyper

function hyper (account, bootstrap) {
	const api = {
		new_task: (opts) => _new_task(account, opts),
		connect: (opts) => _connect(account, bootstrap, opts),
	}
	return api
}
/* --------------------------------------------------------------------------------------------

	NEW TASK

-----------------------------------------------------------------------------------------------*/
async function _new_task (account, { newfeed = true, field, feedkey, topic, log }) { // newfeed false for receivers of feedkey (attester, hoster)
	return new Promise (async (resolve, reject) => {

    log({ type: 'hyper', data: { text: 'starting new task' }})
    var feed
    var stringkey
    var stringtopic
    if (!account.state) account.state = { sockets: {}, feeds: {}, targets: {}, tasks: {} }
    const feeds = account.state.feeds    
    try {
      // 0. If feedkey and/or topic, get stringkey and stringtopic
      if (topic) { 
        topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
        stringtopic = topic.toString('hex') 
      }
      if (feedkey) { // when feed key with or without topic
        feedkey = Buffer.isBuffer(feedkey) ? feedkey : b4a.from(feedkey, 'hex')
        stringkey = feedkey.toString('hex')
        if (!topic) {
          topic = datdot_crypto.get_discoverykey(feedkey)
          stringtopic = topic.toString('hex')
        }
      } 
      
      // 1. Get or make feed
      if (stringtopic && stringkey && feeds[stringtopic]) { // get existing feed
        log({ type: 'hyper', data: { text: 'Existing feed', stringkey, stringtopic }})
        feed = feeds[stringtopic][stringkey].feed 
      } else { // make new feed
        if (newfeed) {  // attester and hoster have to wait to receive feedkey first, so no new feed for them
          if (feedkey) {
            // log({ type: 'hyper', data: { text: 'New feed', feedkey: feedkey.toString('hex') }})
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            if (!field) field = stringtopic
            add_to_feeds({ feeds, field, stringkey, feed, log })
            await feed.ready().catch(safetyCatch)
          } else {
            feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
            // log({ type: 'hyper', data: { text: 'New feed', feedkey: 'none' }})
            await feed.ready().catch(safetyCatch)
            feedkey = feed.key
            stringkey = feedkey.toString('hex')
            if (!topic) topic = datdot_crypto.get_discoverykey(feedkey)
            if (!stringtopic) stringtopic = topic.toString('hex')
            if (!field) field = stringtopic
            add_to_feeds({ feeds, field, stringkey, feed, log })
          }
          log({ type: 'hyper', data: { text: 'New feed', stringkey, field, stringtopic }})
        }
      }
      resolve({ feed })
    } catch(err) {
      log({ type: 'hyper', data: { text: 'Error in _new_task', err } })
      reject(err)
    }
  })
}
/* --------------------------------------------------------------------------------------------

	CONNECT

-----------------------------------------------------------------------------------------------*/
async function _connect ( account, bootstrap, { swarm_opts, targets: Targets, onpeer = noop, done = noop, log }) {
	return new Promise (async (resolve, reject) => {
    try {
      Targets = Object.assign({ targetList: [], msg: {} }, Targets)
      const { targetList, msg, feed } = Targets
      var { role, topic, mode } = swarm_opts
      const { noisePublicKey, noisePrivateKey, state } = account
      var { swarm, tasks, targets, sockets } = state
      var stringtopic

      // log({ type: 'connect', data: { text: 'load-store connect', msg }})

      if (!topic) throw new Error('Error: Can not connect without topic')
      topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
      stringtopic = topic.toString('hex') 
      
      // 2. Make swarm
      if (!swarm) {
        const opts = { 
          bootstrap, 
          keyPair: { publicKey: noisePublicKey, secretKey: noisePrivateKey }, 
          maxParallel: Infinity, 
          maxPeers: Infinity 
        }
        if (!opts.bootstrap) delete opts.bootstrap
        swarm = new hyperswarm(opts)
        account.state.swarm = swarm
        log({ type: 'store connect', data: { text: 'new swarm', mode }})
        swarm.on('connection', onconnection({ account }))
      }


      // 3. Store tasks (for topic)
      add_to_tasks({ tasks, stringtopic, role, log })


      // 4. Join peer through existing channel or join the swarm on topic
      if (!targetList.length) { // Join general topic (if no targetList && no discovery yet)
        // storing general swarm onpeer callback for  each role separately
        // callback for targets is stored spearately under targets[remotestringkey][stringtopic].ontarget

        if (tasks[stringtopic].roles[role].onpeer) tasks[stringtopic].roles[role].onpeer.push(onpeer)
        else tasks[stringtopic].roles[role].onpeer = [onpeer]

        if (!tasks[stringtopic].roles[role].done) tasks[stringtopic].roles[role].done = done
        
        if (!swarm.status(topic)) {
          log({ type: 'store connect', data: { text: 'Join swarm', mode, stringtopic }})
          const discovery = await swarm.join(topic, mode)
          log({ type: 'store connect', data: { text: 'swarm joined', mode, stringtopic }})
          // if (mode.server) {
          //   await discovery.flushed()
          //   log({ type: 'store connect', data: { text: 'discovery flushed', mode, stringtopic }})
          // }
        } else {
          log({ type: 'store connect', data: { text: 'try refresh discovery', mode, stringtopic }})
          mode = await try_refresh_discovery({ swarm, topic, tasks: tasks[stringtopic].roles, log })
        }
      } else { 
        // connect only to the targets on the list
        // topic created with derive-topic is not a valid topic (compressed so not unique anymore)
        // but we connect to targets on targetList directly so the topic is just here for trackings
        for (const remotestringkey of targetList) {
          add_to_targets({ targets, remotestringkey, stringtopic, ontarget: onpeer, role, feed, msg, done, log })			
          // performance attester needs to trigger onconnection cb first, it can't use the existing connection right away
          // if (sockets[remotestringkey] && role !== 'performance_attester') {
          if (sockets[remotestringkey]) {
            const { tid, channel } = sockets[remotestringkey]
            clearTimeout(tid)
            // if (!channel.opened) channel.open()
            const count = sockets[remotestringkey].count
            log({ type: 'store connect', data: { text: 'Target - existing socket', remotestringkey, connections_count: count, channel: channel.opened }})
            // calling handle_target_connection will trigger onpeer cb for all the target topics for this peer
            // so make sure connection is still alive because when onpeer is called it is assumed there is a connection 
            // and task can be executed
            handle_target_connection({ account, remotestringkey, log }) // feed will be returned with ontarget callback
          } else {				
            // Join peer (if no socket connection && no discovery yet)
            log({ type: 'store connect', data: { text: 'Target - joinPeer', remotestringkey, stringtopic, mode }})
            // await swarm.join(topic, mode).flushed()
            await swarm.joinPeer(b4a.from(remotestringkey, 'hex'))
            await swarm.listen() // explicitly start listening for incoming connections
          }
        }
      }
      resolve()
   } catch(err) {
    log({ type: 'hyper', data: { text: 'Error in _connect', err } })
    reject(err)
   }
  })
}
/* --------------------------------------------------------------------------------------------

	ONCONNECTION

-----------------------------------------------------------------------------------------------*/
function onconnection ({ account }) {
	return async (connection, peerInfo) => {
    const { state } = account
    const { targets, tasks, sockets, swarm } = state
    const remotestringkey = peerInfo.publicKey.toString('hex')	
    const conn_log = account.log.sub(`<-onconnection: me: ${account.noisePublicKey.toString('hex').substring(0,5)}, peer: ${remotestringkey.substring(0,5)} `)
   
    const all_topics = get_tasks({ tasks, log: conn_log })
    conn_log({ 
      type: 'onconnection', 
      data: { 
        text: 'onconnection callback', 
        remotestringkey,
        existing_socket: sockets[remotestringkey] ? 'yes' : 'no',
        roles: all_topics.map(stringtopic => JSON.stringify(tasks[stringtopic].roles)),
        conn: all_topics.map(stringtopic => [stringtopic, JSON.stringify(Object.keys(tasks[stringtopic].connections))]), 
        sockets: Object.keys(sockets).map(key => [ key, sockets[key].count ])
      } 
    })

    try {
      // return if peer is trying to connect while new_channel (make_stream_and_channel) is still in progress
      // @TODO: we are trying to fix problem where one peer gets onclose and the other doesn't
      // the one that doesn't before the timeout gets a new onconnection 
      // and tries to reuse the existing channel, while the other peer cleaned the old channel
      // and is on new onconnection creating a new channel
      // we need to return until setTimeout removes old channel
      // if (sockets[remotestringkey] && !sockets[remotestringkey].channel.opened) return

      const tid = setTimeout(() => {
        conn_log({ type: 'onconnection', data: { text: 'ghost connection timeout', remotestringkey }})
        if (sockets[remotestringkey] && !sockets[remotestringkey].count) {
          // we check for count to see if the connection was a match for any of the tasks (count would then not be a zero)
          // if there was no match or process stopped before the replication, then we close the streams
          close_streams_and_channels({ sockets, remotestringkey, log: conn_log })
          delete sockets[remotestringkey]
          conn_log({ type: 'onconnection', data: { text: 'ghost connection removed', remotestringkey }})
        }
      }, DEFAULT_TIMEOUT)
      
      if (!sockets[remotestringkey]) { 
        await make_stream_and_channel({ account, tid, connection, remotestringkey, log: conn_log })
      }
      // this is the target
      if (targets[remotestringkey]) {
        clearTimeout(tid)
        handle_target_connection({ account, remotestringkey, log: conn_log }) 
        // do not return here, further code needs to run too because of ondiscovery cb
      }
      // is peer a client?
      const peer_topics = peerInfo.topics
      if (!peer_topics.length) {
        clearTimeout(tid)
        return
      }
      // peer is a client
      for (const topic of peer_topics) {
        const stringtopic = topic.toString('hex') // only shows topics derived from a feed.discoveryKey
        const discovery = swarm.status(topic)
        conn_log({ type: 'onconnection', data: { text: 'client for topic', stringtopic, remotestringkey }})
        if (
          !discovery || // peer left the topic, but discovery hasn't yet been updated
          !discovery.isClient || // peer is not a client anymore, but discovery mode hasn't been updated yet
          targets[remotestringkey]?.tasks[stringtopic] || // already handled as target connection
          !tasks[stringtopic] || // no task
          // TODO: this only checks if task is being handled by this particular peer
          tasks[stringtopic]?.connections[remotestringkey] // task is already being handled
          ) continue
          clearTimeout(tid)
          handle_matching_topic({ stringtopic, remotestringkey, is_server: false, account, log: conn_log })
      }         
    } catch (err) {
      conn_log({ type: 'Error', data: { text: 'Error: in onconnection', err }})
      throw(err)
    }
	}
}		

async function make_stream_and_channel ({ account, tid, connection, remotestringkey, log }) {
	log({ type: 'onconnection', data: { text: 'loading replication stream and channel', remotestringkey } })
  return new Promise(async (resolve, reject) => {
    try {
      const { tasks, sockets, targets, swarm } = account.state
      var replicationStream
      log({ type: 'onconnection', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
      replicationStream = hypercore.createProtocolStream(connection, { ondiscoverykey })
      mux = hypercore.getProtocolMuxer(replicationStream)
      add_to_sockets({ sockets, remotestringkey, connection, replicationStream, mux, log })
      add_stream_listeners({ state: account.state, remotestringkey, log })
      
      const channel = await new_channel({ account, remotestringkey, log })
      if (!channel) return log({ type: 'exchange-feedkey', data: { text: 'Error: no channel returned', remotestringkey, existing: sockets[remotestringkey].channel ? 'yes' : 'no' }})
      
      resolve({ replicationStream })
      
      // peer is a server
      function ondiscoverykey (discoverykey) { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
        const stringtopic = discoverykey.toString('hex') // only shows topics derived from a feedn.discoveryKey
        const discovery = swarm.status(discoverykey)
        log({ type: 'ondiscovery', data: { text: 'ondiscovery cb', remotestringkey, stringtopic, isServer: discovery?.isServer, isClient: discovery?.isClient } })
        
        if (
          !discovery || // peer left the topic, but discovery hasn't yet been updated
          !discovery.isServer ||  // peer is not a server anymore, but discovery mode hasn't been updated yet
          targets[remotestringkey]?.tasks[stringtopic] || // already handled as target connection
          !tasks[stringtopic] || // no task
          tasks[stringtopic]?.connections[remotestringkey] // task is already being handled
        ) return
        clearTimeout(tid)
        handle_matching_topic({ stringtopic, remotestringkey, is_server: true, account, log })
      }
    } catch (err) {
      log({ type: 'Error', data: { text: 'Error: in loading replication stream and channel', err }})
      reject(err)
    }
  })
}

function handle_matching_topic ({ stringtopic, remotestringkey, is_server, account, log }) {
	const { state } = account
	const { sockets, tasks } = state
	const { channel, replicationStream } = sockets[remotestringkey]
	const string_msg = channel.messages[0]

	if (!tasks[stringtopic]) return
	
	// notify peers in general swarm who they are connected to!!!
	// call onpeer for all the open tasks for this topic

	// TODO - how to know for which task in same topic is this peer?? We can't notify
	var roles = Object.keys(tasks[stringtopic].roles)
	for (const role of roles) {
		// we notify clients, servers get a done() call by the client
    log({ type: 'matching topic', data: { text: 'notifying clients', onpeer_len: tasks[stringtopic].roles[role].onpeer?.length, is_server, role, stringtopic, remotestringkey } })
		if (!tasks[stringtopic].roles[role].onpeer?.length || is_server) continue
		for (const onpeer of tasks[stringtopic].roles[role].onpeer) {
			onpeer({ peerkey: remotestringkey, stringtopic })
		}
	}

	increase_socket_count({ sockets, remotestringkey, log })
	add_to_connections({ tasks, stringtopic, remotestringkey })
	log({ type: 'onconnection', data: { text: 'matching topic', remotestringkey, stringtopic } })
	replicate_feeds({ state, stringtopic, remotestringkey, replicationStream, log })
	subscribe_to_topic_messages({ account, string_msg, stringtopic, remotestringkey, log })
}

function replicate_feeds ({ state, stringtopic, remotestringkey, replicationStream, log }) {
	try {
    const { swarm, feeds, targets } = state
		const status = swarm.status(b4a.from(stringtopic, 'hex'))
		if (!status) return	
		var to_replicate
    const feedkeys = Object.keys(feeds[stringtopic])
    targets[remotestringkey]?.tasks[stringtopic] ?
      to_replicate = feedkeys.filter(key => !targets[remotestringkey]?.tasks[stringtopic]?.feed.key.toString('hex'))
    :
      to_replicate = feedkeys
		for (const key of to_replicate) {
			const feed = feeds[stringtopic][key].feed
			if (!feed.opened || feed.closing) {
				log({ type: 'replicate', data: { text: 'error: feed closing or closed', open: feed.opened, closing: feed.closing, stringtopic, feedkey: feed.key.toString('hex') } })
				continue
			}
			log({ type: 'replicate', data: { text: 'replicate feed', stringtopic, feedkey: feed.key.toString('hex') } })
			feed.replicate(replicationStream)
		}
	} catch (err) {
		log({ type: 'Error', data: { text: 'Error: replicating feeds in the general swarm', err }})
	}
}

async function handle_target_connection ({ account, remotestringkey, log }) { //get feedkey from or send to a target peer (on each custom topic)
	const { state } = account
	const { targets, sockets, tasks } = state
	const { channel, replicationStream } = sockets[remotestringkey]
	const string_msg = channel.messages[0] // get string message type for sending strings through the mux channel with that peer (channel.addMessage(...))

	log({ type: 'onconnection', data: { text: `handle target conn`, remotestringkey, target_tasks: Object.keys(targets[remotestringkey].tasks).length } })				
	
	try {		
		log({ type: 'exchange-feedkey', data: { text: 'load channel', remotestringkey, opened: channel.opened }})
		
		// we need to loop over all the target tasks to find tasks for this peer
		const target_tasks = Object.keys(targets[remotestringkey].tasks)
		for (const stringtopic of target_tasks) {
			try {	
				const { msg, role, feed, ontarget, log } = targets[remotestringkey].tasks[stringtopic]
				const task = tasks[stringtopic]
				if (!task) continue // task might have been cleaned by another process in the meanwhile
				const connections = task.connections
				
				log({ type: 'handle target connection', data: { text: `current peer ${remotestringkey} for topic ${stringtopic}`, target_tasks  } })
				
				// this task is already in progress
				// check for socket too because in case of 2 failed attempts in new_channel onclose, 
				// we will have connections[remotestringkey)] but we will remove sockets[remotestringkey] 
				// so we can create new replicationStream when new connection between same peers is established
				if (connections[remotestringkey] && sockets[remotestringkey]) {  
					log({ type: 'exchange-feedkey', data: { text: 'this task already in progress', remotestringkey }})
					continue
				}

				increase_socket_count({ sockets, remotestringkey, log })
				add_to_connections({ tasks, stringtopic, remotestringkey })
	
				// set registry or check if any messagges are waiting in the queue
				subscribe_to_topic_messages({ account, string_msg, stringtopic, remotestringkey, log })

				if (msg.send) { // see if we need to send or to receive a message
					log({ type: 'protomux', data: { text: `send feedkey` } })
          const tid = setTimeout(() => {
            log({ type: 'protomux', data: { text: 'error: timeout - feedkey-ack not received', remotestringkey, stringtopic }})
            set_status_for_connection({ status: 'feedkey-ack-timeout', tasks, stringtopic, remotestringkey})
            const tasks_with_peer_in_progress = Object.keys(tasks).filter(stringtopic => {
              const status = tasks[stringtopic]?.connections[remotestringkey]?.status
              return status && status === 'in progress'
            })
            if (sockets[remotestringkey] && !tasks_with_peer_in_progress) {
              close_streams_and_channels({ sockets, remotestringkey, log })
              delete sockets[remotestringkey]
              log({ type: 'onconnection', data: { text: 'ghost connection removed', remotestringkey }})
            }
          }, DEFAULT_TIMEOUT)
          tasks[stringtopic].connections[remotestringkey].tid = tid
					string_msg.send(JSON.stringify({ type: 'feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
				} 
				if (role === 'performance_attester') {
					replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
				}
			} catch (err) {
				log({ type: 'error', data: { text: 'Error: handle target connection', err, remotestringkey, stringtopic  } })
			}
		}
	} catch (err) {
		return log({ type: 'exchange-feedkey', data: { text: 'Error: caught in handle target connection', remotestringkey, err }})
	}
}

function subscribe_to_topic_messages ({ account, string_msg, stringtopic, remotestringkey, log }) {
  try {
    var registry = string_msg.registry
    const entry = registry.get(stringtopic)				
    log({ type: 'registry', data: { text: `add to registry and subscribe to topic messages`, stringtopic, remotestringkey, entry: entry ? 'yes' : 'no'   } })
    if (!entry) {
      string_msg.registry.set(stringtopic, { onmessage })
    } else {
      const { msg_queue } = entry
      if (msg_queue) msg_queue.forEach(message => onmessage({ account, message, remotestringkey, string_msg }))
      entry.msg_queue = void 0
      entry.onmessage = onmessage
    }
  } catch (err) {
		log({ type: 'registry', data: { text: 'error in subscribe to topic messages', remotestringkey, stringtopic, err }})
  }
}

function new_channel ({ account, remotestringkey, log }) {
	log({ type: 'protomux', data: { text: `new channel`, remotestringkey } })
	return new Promise(async (resolve, reject) => {
		try {
			const { state } = account
			const { sockets } = state
			var counter = 0
			const opts = { protocol: 'datdot/alpha' }
			var { mux, channel, replicationStream } = sockets[remotestringkey]
			
			if (channel && channel.opened) resolve(channel) // reuse existing opened channel

			mux_proto.make_protocol({ mux, opts, cb, log })
			
			async function cb () {
				counter++
				log({ type: 'protomux', data: { text: 'mux pair cb', remotestringkey, counter, is_channel: channel ? 'yes': 'no', mux_opened: mux.opened(opts) }}) 
				// if (mux.opened(opts)) return reject()
				if (mux.opened(opts)) return

				if (channel && channel.opened) return resolve(channel)
				// else if (channel) channel.close() // close and then repeat the create_and_open_channel, so don't return

				channel = mux_proto.create_and_open_channel({ mux, opts: {...opts, onopen, onclose }, log })
				if (!channel) {
					log({ type: 'protomux', data: { text: 'Error: mux_proto returned no channel' }}) 
					return
				}
				sockets[remotestringkey].channel = channel				

				// add message types for different encodings (i.e. string...) + intercept onmessage with a dispacher
				var string_msg = channel.messages[0] 
				if (!string_msg) string_msg = channel.addMessage({ encoding: c.string, onmessage: dispacher })
				
				// load registry to store onmessage callbacks and undelivered messages (msg_queue) for each stringtopic
				var registry = string_msg.registry
				if (!registry) registry = string_msg.registry = new Map()

				// dispacher stores the message in the queue until we can process it
				// example: chain emits the event bob receives it and sends the message, but alice hasn't received the event yet
				function dispacher (message) {
					const { stringtopic } = JSON.parse(message)
					const entry = registry.get(stringtopic)
					log({ type: 'dispacher', data: { text: 'dispacher received', message, entry: entry ? 'yes' : 'no' }}) 
					if (!entry) {
						return registry.set(stringtopic, { msg_queue: [message] })		
					} 
					const { onmessage, msg_queue } = entry
					onmessage ? onmessage({ account, message, remotestringkey, string_msg, state }) : msg_queue.push(message)
				}
			}	

			function onopen () { 
				log({ type: 'protomux', data: { text: 'onopen: either side opened channel', counter_onopen: counter, mux_opened: mux.opened({ protocol: 'datdot/alpha' }) }}) 
				counter = 0 // we need to reset it so that it catches the 2 consecutive fails in onclose
				return resolve(channel)
			}
			async function onclose (isRemote, channel) { 
				log({ type: 'protomux', data: { text: 'onclose: either side closed channel', counter_onclose: counter, opened: channel.opened, closed: channel.closed }})
				channel.close() // notify other side, maybe they closed but are unaware
				// we don't reject here, because we need to wait for the second cb in this case
			}

		} catch (err) {
			log({ type: 'protomux', data: { text: `Error: new channel`, remotestringkey } })
			return reject(err)
		}
	})
}

async function onmessage ({ account, message, remotestringkey, string_msg }) {
	const { state } = account
	const { type, challenge_id, hosterkey, feedkey, role, stringtopic, proof_of_contact } = JSON.parse(message)
	const { feeds, targets, sockets, tasks } = state
	const { replicationStream } = sockets[remotestringkey]
	const { ontarget, done, log } = targets[remotestringkey]?.tasks[stringtopic] || { log: account.log.sub(`performance hoster, challenge: ${challenge_id}`) }
	// log({ type: 'onmessage', data: { text: 'new message received', role, type } })

	var feed
	if (type === 'feedkey') {	// make feed
		log({ type: 'onmessage protomux', data: { text: 'received feedkey', feedkey: feedkey.toString('hex') }})
		feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
    await feed.ready().catch(safetyCatch)
		const stringkey = feedkey.toString('hex')
		add_to_feeds({ feeds, field: stringtopic, stringkey, feed, log })
		string_msg.send(JSON.stringify({ type: 'ack-feedkey', stringkey, stringtopic }))
		replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
	} 
	else if (type === 'ack-feedkey') {
    const { tid } = tasks[stringtopic].connections[remotestringkey]
    console.log({ name: log.path, conn: tasks[stringtopic].connections[remotestringkey] })
    clearTimeout(tid)
    delete tasks[stringtopic].connections[remotestringkey].tid
		feed = targets[remotestringkey].tasks[stringtopic].feed
		log({ type: 'onmessage protomux', data: { text: 'received ack', feedkey: feed.key.toString('hex') }})
		replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
	}
	else if (type === 'requesting-proof-of-contact') {
		// send proof
		const data = b4a.from(challenge_id.toString(), 'binary')
		const proof_of_contact = account.sign(data)
		log({ type: 'onmessage', data: { text: 'type: requesting-proof-of-contact', challenge_id, message } })
		const hosterkey = account.identity.noiseKey.toString('hex')
		string_msg.send(JSON.stringify({ type: 'proof-of-contact', stringtopic, hosterkey, proof_of_contact: proof_of_contact.toString('hex') }))
	}
	else if (type === 'proof-of-contact') {
	  log({ type: 'proof of contact', data: { proof_of_contact, hosterkey } })
    // calling done for 'performance_attester/storage_attester'
		done({ type, proof: proof_of_contact, hosterkey })
	}
	else if (type === 'done') {
		// peer has already closed the task, but the callback is still stored in the onpeer array
		if (!tasks[stringtopic]) return 
		const mykey = account.noisePublicKey.toString('hex')
		// others => type done
		
		if (done) { // calling done for targets
			log({ type, data: { text: 'calling done for target task', stringtopic, type } })
			// targets store onpeer under targets[remotestringkey]?.tasks[stringtopic].onpeer
			done({ type, stringtopic, peerkey: mykey }) 
		} else { // calling done for general swarm tasks
			log({ type, data: { text: 'calling done for no target task', stringtopic, type, role } })
			// it can be one or many roles (i.e. author and hoster2author) and we check what cb we have in tasks[stringtopic][role].done 
      // done msg is send in done-task-cleanup (notify_to_close_task)
			if (role.includes('author')) {
				tasks[stringtopic].roles['author'].done({ peerkey: remotestringkey, stringtopic })
			}
			// if peer has h2a & hoster role, let's not call with h2a, because
			// it will set h2a to hoster in remove_from_roles()
			// & if task hasn't been done yet, h2a will stop to lookup
			// TODO: find a solution for it
			else if (role.includes('hoster2author')) {
				tasks[stringtopic].roles['hoster2author'].done({ role, peerkey: remotestringkey, stringtopic })
			}
			else if (role.includes('hoster')) {
				tasks[stringtopic].roles['hoster'].done({ role, peerkey: remotestringkey, stringtopic })
			}
		}
	}
}		

function replicate_to_target ({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log }) {
	if (!feed.opened || feed.closing) return
	feed.replicate(replicationStream)
	log({ type: 'replicate', data: { text: 'replicate to target', stringtopic  }})
	ontarget({ feed, remotestringkey })
}
/*-----------------------------------------------------------------------------------------------------
		
	HELPERS
		
------------------------------------------------------------------------------------------------------*/
function add_to_tasks ({ tasks, stringtopic, role, log }) {
	if (!tasks[stringtopic]) {
		tasks[stringtopic] = { roles: { [`${role}`]: { count: 1 } }, connections: {} } 
		log({ type: 'hyper', data: { text: 'New topic', role, stringtopic }})
	} else {
		tasks[stringtopic].roles[role]?.count ? tasks[stringtopic].roles[role].count++ : tasks[stringtopic].roles[role] = { count: 1 }
		log({ type: 'hyper', data: { text: 'Existing topic', role, stringtopic, tasks: tasks[stringtopic] }})
	}
}

function add_to_targets ({ targets, remotestringkey, stringtopic, role, ontarget, feed, msg, done, log }) {
	if (!targets[remotestringkey]) targets[remotestringkey] = { tasks: { [`${stringtopic}`]: { ontarget, role, feed, msg, done, log } } }
	else targets[remotestringkey].tasks[stringtopic] = { ontarget, role, feed, msg, done, log }
}
		
function add_to_sockets ({ sockets, remotestringkey, connection, replicationStream, mux, log }) {
	log({ type: 'hyper', data: { text: 'Adding to sockets', remotestringkey, does_exist: sockets[remotestringkey] ? 'yes' : 'no' }})
	sockets[remotestringkey] = { socket: connection, replicationStream, mux }
  // count is undefined and will be set to 1 once there is a task match
}

function increase_socket_count ({ sockets, remotestringkey, log}) {
	sockets[remotestringkey].count ? sockets[remotestringkey].count++ : sockets[remotestringkey].count = 1
	log({ type: 'socket', data: { text: 'Increase count', remotestringkey, count: sockets[remotestringkey].count }})
}

function add_to_connections ({ tasks, stringtopic, remotestringkey }) {
	set_status_for_connection({ status: 'in progress', tasks, stringtopic, remotestringkey})
}

function add_to_feeds ({ feeds, field, stringkey, feed, log }) {
	if (!feeds[field]) feeds[field] = {}
	if (!feed) return
	log({ type: 'feeds', data: { text: 'Add to feeds', field, stringkey }})
	feeds[field] = { [stringkey]: { feed } }
}

function add_stream_listeners ({ state, remotestringkey, log }) {
	const { sockets } = state
	const { replicationStream } = sockets[remotestringkey]

	// replicationStream.on('error', (err) => { 
	// 	log({ type: 'replicationStream on', data: { text: `on error: replicationStream error`, err, code: err.code, stack: err.stack /*snapshot: get_snapshot(account, remotestringkey)*/ } })
	// })

	replicationStream.on('close', () => { 
		log({ type: 'replicationStream on', data: { text: `on close: replicationStream closed`, remotestringkey, socket_exist: sockets[remotestringkey] ? 'yes' : 'no', count: sockets[remotestringkey]?.count } }) 
		if (sockets[remotestringkey]) {
      handle_replicationStream_onclose({ state, remotestringkey, log })
		}
	})
}

async function handle_replicationStream_onclose ({ state, remotestringkey, log }) {
  const { swarm, sockets, targets, tasks, feeds } = state
  
  // error occured immediatelly after onconnection & loading of the streams
  const { count } = sockets[remotestringkey]
  if (!count) {
    log({ type: 'replicationStream_onclose', data: { text: `replicationStream closed`, remotestringkey, socket_exist: sockets[remotestringkey] ? 'yes' : 'no', count: sockets[remotestringkey]?.count } }) 
    close_streams_and_channels({ sockets, remotestringkey, log })
    delete sockets[remotestringkey]
    return
  }
	
	const all_topics = get_tasks({ tasks, log })

	log({ type: 'replicationStream_onclose', data: { 
		text: `handle_replicationStream_onclose start`, 
		remotestringkey,
		all_topics: all_topics,
		roles: all_topics.map(stringtopic => JSON.stringify(tasks[stringtopic].roles)),
		conn: all_topics.map(stringtopic => Object.keys(tasks[stringtopic].connections)), 
		sockets: Object.keys(sockets).map(key => [ key, sockets[key].count ])
	} }) 
	
	for (const stringtopic of all_topics) {
		
		const topic = b4a.from(stringtopic, 'hex')
		const task = tasks[stringtopic]
		if (!task) continue // task might have been cleaned by another process in the meanwhile
		if (tasks[stringtopic].connections[remotestringkey]) {
      
      // --------- TARGET ------------
			// stringtopic is one time used & is unique 
			if (
        tasks[stringtopic].roles['encoder2attester']?.count || 
				tasks[stringtopic].roles['storage_hoster']?.count || 
				tasks[stringtopic].roles['hoster2attester']?.count
				) {
        if (tasks[stringtopic].connections[remotestringkey] === 'in cleanup') continue
        set_status_for_connection({ status: 'in cleanup', tasks, stringtopic, remotestringkey})
				// TASKS
				remove_from_tasks({ tasks, stringtopic, log })
				// TARGETS
				remove_from_target_tasks({ targets, stringtopic, remotestringkey, log }) // we have always only one task per stringtopic for targets
				const target_tasks = get_target_tasks({ targets, remotestringkey, log })
				if (!target_tasks.length) {
					log({ type: 'handle cleanup', data: { text: 'remove from targets', role, remotestringkey, stringtopic  }})
					remove_from_targets({ targets, remotestringkey, log })
					// remove_from_registry({ sockets, stringtopic, remotestringkey, log })
				}
				// FEEDS
				close_feeds({ feeds, field: stringtopic, log })
				//SWARM
        swarm.leavePeer(b4a.from(remotestringkey, 'hex')) // stop attempting direct connections to the peer
				// SOCKETS
				decrease_socket_count({ sockets, remotestringkey, log })
				const { count } = sockets[remotestringkey]
				if (!count) {
          close_streams_and_channels({ sockets, remotestringkey, log })
					delete sockets[remotestringkey]
				}
				if (tasks[stringtopic]?.connections[remotestringkey]) {
          set_status_for_connection({ status: 'cleaned up', tasks, stringtopic, remotestringkey})
				}
			}
			// --------- NO TARGET/GENERAL SWARM -------------
			else if (
        tasks[stringtopic].roles['author']?.count || 
				tasks[stringtopic].roles['hoster']?.count || 
				tasks[stringtopic].roles['hoster2author']?.count
				) {
          // TASKS
          var role 
          if (tasks[stringtopic].roles['author']?.count) role = 'author'
          else if (tasks[stringtopic].roles['hoster']?.count) role = 'hoster'
          else if (tasks[stringtopic].roles['hoster2author']?.count) role = 'hoster2author'
          
          await clean_general_task({ state, role, stringtopic, peers: [remotestringkey], log })
          if (tasks[stringtopic]?.connections[remotestringkey]) {
            set_status_for_connection({ status: 'cleaned up', tasks, stringtopic, remotestringkey})
          }
        } else {
          // cleanup was triggered due to connection error, peers will try to reconnect and in order to not think
          // task is still in progres, we set it to null
          // if peer disconnected before the connection was added to task connections and sockets, it will not get to handle done task,
          // but if it was, it will trigger this function
          log({ type: 'handle cleanup', data: { text: 'remove from targets', role, remotestringkey, stringtopic  }})
				set_status_for_connection({ status: null, tasks, stringtopic, remotestringkey})
			}
		}
	
    log({ type: 'replicationStream_onclose', data: { 
      text: `handle_replicationStream_onclose done`, 
      remotestringkey,
      stringtopic,
      target: targets[remotestringkey] ? 'yes' : 'no',
      all_topics: get_tasks({ tasks, log }),
      conn: Object.keys(tasks).map(stringtopic => Object.keys(tasks[stringtopic].connections)), 
      sockets: Object.keys(sockets).map(key => [ key, sockets[key].count ])
    } }) 
	}

}

function noop () {}

/* --------------------------------------------------------------------------------------------

	state

-----------------------------------------------------------------------------------------------*/

/* 
account = {
	storage: {
			feedkey: { feed, db, discovery, unintercept }
	},
	state:  {
			swarm,
			sockets: {
					[remotestringkey]: { 
							count: 1 // connection count (when connection is established, for every task with this peer)
							socket, 
							replicationStream,
							mux,
							channel: {
								messages: {
									// different types, our only type for now is string_msg (channel.messages[0])
									registry: { msg_queue: [message], onmessage }
									onmessage: dispacher
								}
							},
					}
			},
			tasks: { // general swarm tasks or target tasks
				[stringtopic]: {
					connections: {
						[remotestringkey]: { status: 'in progress' or 'in cleanup', tid }
					},
					roles: {
						author: { count: 0, onpeer: [], done: [] } // { server: true }
						sponsor: { count: 0, onpeer, done }, // { client: true } 
						encoder2author: { count: 1, onpeer, done }, // { client: true }
						encoder2attester: { count: 0, onpeer, done }, // { server: true }
						attester2encoder: { count: 0, onpeer, done }, // { client: true }
						attester2hoster: { count: 0, onpeer, done }, // { server: true }
						hoster2author: { count: 0, onpeer, done }, // { server: true, client: true }
						hoster2attester: { count: 0, onpeer, done }, // { client: true }
						hoster: { count: 0, onpeer, done }, // { server: true }
						performance_attester: { count: 0, onpeer, done }, { client: true }
						storage_attester: { count: 0, onpeer, done } { client: true }
					}
				}
			}
			feeds: {
				[field]: { //stringtopic or custom for performance challenges
					[stringkey]: { feed }
				}
			}
			targets: {  // subset of tasks
				[remotestringkey]: { // rename to stringremotekey
					tasks: {
						[stringtopic1]: { feed, msg: { receive, send }, ontarget, role, done, status, log }, // status set when replication starts
						[stringtopic2]: { feed, msg: { receive, send }, ontarget, role, done, status, log }
					}
				}
			},
			// targets are created only when we have a msg (send or receive) and a targetList and we need to exchange keys first and then replicate
	},
} 

*/