const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const RAM = require('random-access-memory')
const datdot_crypto = require('datdot-crypto')
const mux_proto = require('_datdot-service-helpers/mux-proto')
const b4a = require('b4a')
const c = require('compact-encoding')
const try_refresh_discovery = require('_datdot-service-helpers/try-refresh-discovery')
const safetyCatch = require('safety-catch')



module.exports = hyper

function hyper (account, bootstrap) {
	const api = {
		load_feed: (opts) => _load_feed(account, opts),
		connect: (opts) => _connect(account, bootstrap, opts),
	}
	return api
}

function noop () {}

/* --------------------------------------------------------------------------------------------

																				LOAD FEED

-----------------------------------------------------------------------------------------------*/
async function _load_feed(account, { make = true, feedkey, topic, log }) { // newfeed false for receivers of feedkey (attestor, hoster)
	// log({ type: 'hyper', data: { text: 'loading feed' }})
	var feed
	var stringkey
	var stringtopic
	if (!account.cache) account.cache = { sockets: {}, feeds: {}, targets: {}, tasks: {} }
	const feeds = account.cache.feeds
	
	// 0. If feedkey and/or topic, get stringkey and stringtopic
	if (topic) { // when msg.receive or msg.send (no feedkey, only topic)
		topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
		stringtopic = topic.toString('hex') 
	}
	if (feedkey) { // when feed key with or without topic
		feedkey = Buffer.isBuffer(feedkey) ? feedkey : b4a.from(feedkey, 'hex')
		stringkey = feedkey.toString('hex')
		if (!topic) {
			topic = datdot_crypto.get_discoverykey(feedkey)
			stringtopic = topic.toString('hex')
		}
	} 
	
	// 1. Get or make feed
	if (stringtopic && stringkey && feeds[stringtopic]) { // get existing feed
		log({ type: 'hyper', data: { text: 'Existing feed', stringkey, stringtopic }})
		feed = feeds[stringtopic][stringkey].feed 
	} else { // make new feed
		if (make) {  // attestor and hoster have to wait to receive feedkey first, so make is false for them
			if (feedkey) {
				// log({ type: 'hyper', data: { text: 'New feed', stringkey: feedkey.toString('hex') }})
				feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
				await feed.ready().catch(safetyCatch)
			} else {
				feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
				// log({ type: 'hyper', data: { text: 'New feed', stringkey: 'none' }})
				await feed.ready().catch(safetyCatch)
				feedkey = feed.key
				stringkey = feedkey.toString('hex')
				if (!topic) topic = datdot_crypto.get_discoverykey(feedkey)
				if (!stringtopic) stringtopic = topic.toString('hex')
			}
			log({ type: 'hyper', data: { text: 'New feed', stringkey, stringtopic }})
		}
		// 2. Store topic and new feed (if available)
		feeds[stringtopic] = {}  // add topic to the feeds
		if (feed) feeds[stringtopic] = { [stringkey]: { feed } }
	}

	return { feed }

	
}

/* --------------------------------------------------------------------------------------------

																				CONNECT

-----------------------------------------------------------------------------------------------*/
async function _connect ( account, bootstrap, { swarm_opts, targets: Targets, log }) {
	Targets = Object.assign({ targetList: [], ontarget: noop, done: noop, msg: {} }, Targets)
	const { targetList, ontarget, msg, feed, done } = Targets
	var { role, topic, mode } = swarm_opts
	const { noisePublicKey, noisePrivateKey, cache } = account
	var { swarm, tasks, targets, sockets } = cache
	var stringtopic
	
	// log({ type: 'connect', data: { text: 'load-store connect', msg }})

	if (!topic) throw new Error('Error: Can not connect without topic')
	topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
	stringtopic = topic.toString('hex') 

	// 2. Make swarm
	if (!swarm) {
		swarm = new hyperswarm({ 
			bootstrap, 
			keyPair: { publicKey: noisePublicKey, secretKey: noisePrivateKey }, 
			maxParallel: Infinity, 
			maxPeers: Infinity 
		})
		account.cache.swarm = swarm
		log({ type: 'store connect', data: { text: 'new swarm', mode }})
		swarm.on('connection', onconnection(account, log))
	}

	// 3. Store tasks (for topic)
	if (!tasks[stringtopic]) {
		tasks[stringtopic] = { roles: { [`${role}`]: 1 }, connections: {} } 
		log({ type: 'hyper', data: { text: 'New topic', role, stringtopic }})
	} else {
		tasks[stringtopic].roles[`${role}`] ? tasks[stringtopic].roles[`${role}`]++ : tasks[stringtopic].roles[`${role}`] = 1
		log({ type: 'hyper', data: { text: 'Existing topic', role, stringtopic, tasks: tasks[stringtopic] }})
	}
	

	// 3. Join peer through existing channel or join the swarm on topic
	const len = targetList.length
	if (!len) { // Join topic (if no targetList && no discovery yet)
		if (!swarm.status(topic)) {
			log({ type: 'store connect', data: { text: 'Join swarm', mode, stringtopic }})
			await swarm.join(topic, mode).flushed()
		} else {
			log({ type: 'store connect', data: { text: 'try refresh discovery', mode, stringtopic }})
			mode =  await try_refresh_discovery({ swarm, topic, tasks: tasks[stringtopic].roles, log })
		}
	} else { // connect only to the targets on the list
		// topic created with derive-topic is not a valid topic
		// but we connect to targets on targetList directly so the topic
		for (var i = 0; i < len; i++) {
			const remotestringkey = targetList[i]
			if (!targets[remotestringkey]) targets[remotestringkey] = { topics: { [stringtopic]: { ontarget, feed, msg, done, log } } }
			else targets[remotestringkey].topics[stringtopic] = { ontarget, feed, msg, done, log }
									
			if (sockets[remotestringkey]) {
				log({ type: 'store connect', data: { text: 'Target - existing socket', connections_count: sockets[remotestringkey].count }})
				exchange_feedkey({ account, remotestringkey, log }) // feed will be returned with ontarget callback
			} else {				
				// Join peer (if no socket connection && no discovery yet)
				log({ type: 'store connect', data: { text: 'Target - joinPeer', remotestringkey, stringtopic, mode }})
				await swarm.joinPeer(b4a.from(remotestringkey, 'hex'))
				await swarm.listen() // explicitly start listening for incoming connections
			}
		}
	}
	
}


/* --------------------------------------------------------------------------------------------

																				ONCONNECTION

-----------------------------------------------------------------------------------------------*/
function onconnection (account, conn_log) {
	return async (connection, peerInfo) => {
		try {
			const { cache } = account
			const { swarm, targets, tasks, sockets } = cache
			const remotekey = peerInfo.publicKey
			const remotestringkey = remotekey.toString('hex')	
			const conn_log = account.log.sub(`<-onconnection: me: ${account.noisePublicKey.toString('hex').substring(0,5)}, peer: ${remotestringkey.substring(0,5)} `)
			conn_log({ type: 'onconnection', data: { text: 'onconnection callback', remotestringkey } })

			const target = targets[remotestringkey]

			const { replicationStream } = await load_replicationStream_and_channel({ account, connection, remotestringkey, log: conn_log })
			// // make sure you only connect once to the same peer, but keep the count
			
			// see if this peer is on the list of targets you need to connect to for a specific task
			if (target) {
				conn_log({ type: 'onconnection', data: { text: 'Target', remotestringkey } })
				return exchange_feedkey({ account, remotestringkey, log: conn_log }) // this is the peer we need to exchange feedkeys with
			}
			// handle client (isInitiator is true)
			if (peerInfo.topics.length) {
				for (var i = 0, len = peerInfo.topics.length; i < len; i++) {
					const topic = peerInfo.topics[i]
					const stringtopic = topic.toString('hex') // only shows topics derived from a feed.discoveryKey
	
					if (tasks[stringtopic]) { // see if we have a task for this topic
						const connections = tasks[stringtopic].connections
						if (connections[remotestringkey]) continue
						connections[remotestringkey] = true
						sockets_count({ sockets, remotestringkey, log: conn_log })
						conn_log({ type: 'onconnection', data: { text: 'Matching topic - client', stringtopic, remotestringkey } })
						replicate_feeds({ cache, stringtopic, remotestringkey, replicationStream, log: conn_log })
					}
				}
			} 
		} catch (err) {
			safetyCatch(err)
			conn_log({ type: 'Error', data: { text: 'Error: in onconnection', err }})
		}
	}
}		

/*-----------------------------------------------------------------------------------------------------
		
																										HELPERS
		
		------------------------------------------------------------------------------------------------------*/

	function sockets_count ({ sockets, remotestringkey, log}) {
		sockets[remotestringkey].count++
		log({ type: 'socket', data: { text: 'Increase count', remotestringkey, count: sockets[remotestringkey].count   }})
	}
	
	async function load_replicationStream_and_channel ({ account, connection, remotestringkey, log }) {
		log({ type: 'onconnection', data: { text: 'loading replication stream and channel', remotestringkey } })
		try {
			const { tasks, sockets, targets } = account.cache
			const target = targets[remotestringkey]
			var replicationStream
			var mux
	
			if (sockets[remotestringkey]) {
				log({ type: 'onconnection', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
				replicationStream = sockets[remotestringkey].replicationStream
				mux = sockets[remotestringkey].mux
			} else {
				log({ type: 'onconnection', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
				replicationStream = hypercore.createProtocolStream(connection, { ondiscoverykey })
				mux = hypercore.getProtocolMuxer(replicationStream)
				
				sockets[remotestringkey] = { socket: connection, replicationStream, mux, count: 0 }
				
				replicationStream.on('error', (err) => { 
					//err: [Error: connection reset by peer] { code: 'ECONNRESET' },
					// safetyCatch(err)
					// log({ type: 'replicationStream on', data: { text: `on error: replicationStream error`, err, stack: err.stack /*snapshot: get_snapshot(account, remotestringkey)*/ } })
				})
				replicationStream.on('close', () => { 
					log({ type: 'replicationStream on', data: { text: `on close: replicationStream closed` } }) 
					close_stuff({account, remotestringkey, log}) 
				})
				replicationStream.on('end', () => { 
					log({ type: 'replicationStream on', data: { text: `on end: replicationStream ended` } })
					close_stuff({account, remotestringkey, log})
				})
				replicationStream.on('timeout', (err) => { log({ type: 'replicationStream on', data: { text: `ontimeout: replicationStream timed out` } }) })
				replicationStream.on('data', (data) => { 
					// const err = new Error
					// console.log('replication stream on data', data, data.toString())
					// log({ type: 'replicationStream on', data: { text: `on data: replicationStream`, data, stack: err.stack } }) 
				})
			
			}
	
			return { replicationStream }

			// handle server (isInitiator is false)
			function ondiscoverykey (discoverykey) { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
				if (target) return // if peer from the targetList, don't do anything
				const stringtopic = discoverykey.toString('hex') // only shows topics derived from a feed.discoveryKey
				// see if there is task for this topic and that we are not yet connected to the peer for this topic
				if (tasks[stringtopic]) {
					const connections = tasks[stringtopic].connections
					if (connections[remotestringkey]) return
					connections[remotestringkey] = true
					sockets_count({ sockets, remotestringkey, log })
					log({ type: 'ondiscovery', data: { text: 'matching topic - server', remotestringkey, stringtopic } })
					replicate_feeds({ cache: account.cache, stringtopic, remotestringkey, replicationStream, log })
				}
			}
		} catch (err) {
			safetyCatch(err)
			log({ type: 'Error', data: { text: 'Error: in loading replication stream and channel', err }})
		}
	}
	
	function replicate_feeds ({ cache, stringtopic, remotestringkey, replicationStream, log }) {
		try {
			const { swarm, feeds, sockets }  = cache
			const topic = b4a.from(stringtopic, 'hex')
			const mux = sockets[remotestringkey].mux

			const status = swarm.status(topic)
			if (!status) return
			
			// log({ type: 'replicate feeds', data: { text: 'replicate', stringtopic, feeds: Object.keys(feeds[stringtopic]).length } })
			
			const keys = Object.keys(feeds[stringtopic])
			const len = keys.length
			for (var i = 0; i < len; i++) {
				const key = keys[i]
				const feed = feeds[stringtopic][key].feed
				log({ type: 'onconnection', data: { text: 'replicate feed', stringtopic, feedkey: feed.key.toString('hex') } })
				if (!feed.opened || feed.closing) continue
				feed.replicate(replicationStream, { session: true })
			}
		} catch (err) {
			safetyCatch(err)
			log({ type: 'Error', data: { text: 'Error: replicating feeds in the general swarm', err }})
		}
	}

	async function exchange_feedkey ({ account, remotestringkey, log }) { //get feedkey from or send to a target peer (on each custom topic)
		const { cache } = account
		const { targets, sockets, tasks, feeds } = cache
		const target_topics = Object.keys(targets[remotestringkey].topics)
		const len = target_topics.length
		const { replicationStream } = sockets[remotestringkey]
		log({ type: 'onconnection', data: { text: `exchange feedkey`, remotestringkey, target_topics: len } })				
		
		sockets_count({ sockets, remotestringkey, log })
		
		const channel = await new_channel({ account, remotestringkey, log }).catch(err => {
		  log({ type: 'exchange-feedkey', data: { text: 'Error: in new channel', remotestringkey, err }})
			return
		})
		if (!channel) return log({ type: 'exchange-feedkey', data: { text: 'Error: channel', remotestringkey }})
		log({ type: 'exchange-feedkey', data: { text: 'new channel returned', remotestringkey, opened: channel.opened }})

		// load registry
		const string_msg = channel.messages[0] // get string message type for sending strings through the mux channel with that peer (channel.addMessage(...))
		var registry = string_msg.registry
		
		// we need to loop over all the topics for this peer we just got connected to 
		// in order to find all the topics we have to connect for (peerInfo.topics only shows
		// topics derived from a feedkey, not custom topics
		
		for (var i = 0; i < len; i++) {
		  try {
		    const stringtopic = target_topics[i]
				const { feedkeysent, msg, log } = targets[remotestringkey].topics[stringtopic]

				const connections = tasks[stringtopic].connections
				if (connections[remotestringkey]) continue
				connections[remotestringkey] = true
							
				log({ type: 'exchange feedkey', data: { text: `current peer ${remotestringkey} for topic ${stringtopic}`, target_topics, feedkeysent, len  } })
				if (feedkeysent) {
					keysent_count++
					continue // we already exchanged key for this topic
				}
				// get entry for stringtopic
				const entry = registry.get(stringtopic)				
				if (entry) {
					log({ type: 'exchange feedkey', data: { text: `entry exists`, stringtopic, remotestringkey   } })
					const { msg_queue } = entry
					if (msg_queue) msg_queue.forEach(onmessage)
					entry.msg_queue = void 0
					entry.onmessage = onmessage
				} else {
					log({ type: 'exchange feedkey', data: { text: `entry new`, stringtopic, remotestringkey   } })
					string_msg.registry.set(stringtopic, { onmessage })
				}		
				if (msg.send) { // see if for this topic we send or receive a message
					const { feed } = targets[remotestringkey].topics[stringtopic]
					log({ type: 'protomux', data: { text: `send feedkey` } })
					string_msg.send(JSON.stringify({ type: 'feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
				} 
				// else if (msg.receive) {
				// 	log({ type: 'onconnection', data: { text: `receiving the message` } })
				// }
			} catch (err) {
				safetyCatch(err)
				log({ type: 'error', data: { text: 'Error: exchange feedkey', err, remotestringkey, stringtopic  } })
			}
		}

		async function onmessage (message) {
			const { type, feedkey, stringtopic, proof_of_contact } = JSON.parse(message)
			const { ontarget, done, log } = targets[remotestringkey].topics[stringtopic]
			console.log('new messssage', targets[remotestringkey].topics[stringtopic])

			var feed
			if (type === 'feedkey') {	
				// 1. Make feed
				log({ type: 'onmessage protomux', data: { text: 'received feedkey', feedkey: feedkey.toString('hex') }})
				feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
				await feed.ready().catch(safetyCatch)
				// feedkey = feed.key
				stringkey = feedkey.toString('hex')
				feeds[stringtopic][stringkey] = { feed }
				// log({ type: 'onmessage protomux', data: { text: 'New feed - exchange key', stringkey }})
				string_msg.send(JSON.stringify({ type: 'ack-feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
				replicate_to_peer({ cache, ontarget, feed, replicationStream, stringtopic, log })
			} 
			else if (type === 'ack-feedkey') {
				feed = targets[remotestringkey].topics[stringtopic].feed
				const stringkey = feed.key.toString('hex')
				log({ type: 'onmessage protomux', data: { text: 'received ack', stringkey }})
				replicate_to_peer({ cache, ontarget, feed, replicationStream, stringtopic, log })
			}
			else if (type === 'proof-of-contact') {
				console.log('proof of contact', proof_of_contact )
				string_msg.send(JSON.stringify({ type: 'ack-proof-of-contact', stringtopic }))
				done(proof_of_contact)
			}
			else if (type === 'ack-proof-of-contact') {
				console.log('ack-proof-of-contact', stringtopic )
				done()
			}

			function replicate_to_peer ({ ontarget, feed, replicationStream, stringtopic, log }) {
				if (!feed.opened || feed.closing) return
				feed.replicate(replicationStream, { session: true })
				targets[remotestringkey].topics[stringtopic].feedkeysent = true 
				log({ type: 'replicate', data: { text: 'replicate to target', stringtopic  }})
				ontarget({ feed, remotekey: remotestringkey })
			}
		}		
	}

	function new_channel ({ account, remotestringkey, log }) {
		return new Promise(async (resolve, reject) => {
			try {
				const sockets = account.cache.sockets
				var { mux, channel } = sockets[remotestringkey]
				log({ type: 'open protomux channel', data: { text: `new channel`, remotestringkey } })
				if (channel) {
					log({ type: 'protomux', data: { text: 'channel already exists' }})
					if (channel.closed) channel.open()
					return resolve(channel)
				}
				
				// make a protocol
				const opts = { protocol: 'datdot/alpha', /*id: make_id_buff()*/ }
				mux_proto.make_protocol(mux, opts, cb, log)

				async function cb () {
					log({ type: 'protomux', data: { text: 'new channel cb', remotestringkey }}) 
					
					// add message types for different encodings (i.e. string...)
					const messages = [{ encoding: c.string, onmessage: dispacher }]
					
					channel = mux_proto.create_and_open_channel(mux, {...opts, onopen, onclose, messages }, log)
					if (!channel) return reject()
					
					sockets[remotestringkey].channel = channel				

					log({ type: 'protomux', data: { text: 'add registry to string_msg', remotestringkey }}) 
					const string_msg = channel.messages[0]
					// load registry to store onmessage callbacks and undelivered messages (msg_queue) for each stringtopic
					var registry = string_msg.registry
					if (!registry) registry = string_msg.registry = new Map()
										
					// intercept onmessage with dispacher that will call the right callback
					function dispacher (message) {
						const { stringtopic } = JSON.parse(message)
						const reg = registry.get(stringtopic)
						if (reg) {
							log({ type: 'dispacher', data: { text: 'dispacher reg - existing', stringtopic }}) 
							const { onmessage, msg_queue } = reg
							onmessage ? onmessage(message) : msg_queue.push(message)
						} 
						else {
							log({ type: 'dispacher', data: { text: 'dispacher reg - new', stringtopic }}) 
							registry.set(stringtopic, { msg_queue: [message] })		
						}
						log({ type: 'dispacher', data: { text: `dispacher new message`, message, stringtopic, registry: [...registry.entries()] } })
					}
				}	

				function onopen () { 
					log({ type: 'protomux', data: { text: 'onopen: either side opened channel' }}) 
					if (channel.closed) console.log('actually closed')
					return resolve(channel)
				}
				function onclose (isRemote, channel) { 
					log({ type: 'protomux', data: { text: 'onclose: either side closed channel', isRemote, /*channel*/ }}) 
					// for some reason you can get onclose even if you get onopen later
				}

				const make_id_buff = () => { // unique for each pair, consists of both remotekeys
					const mystringkey = account.noisePublicKey.toString('hex')
					var arr = [mystringkey, remotestringkey]
					arr = arr.sort((a,b) => a < b ? -1 : 1)
					arr.reduce((a, b) => a + b, 0)
					return b4a.from(arr) 
				}

			} catch (err) {
				log({ type: 'protomux', data: { text: `Error: new channel`, remotestringkey } })
				reject()
			}
		})
	}

	function close_stuff ({account, remotestringkey, log}) {
		return
		// TODO: replace this with targeted feed.onclose, channel.onclose listeners and closing when that triggered
		const {cache} = account
		const { swarm, sockets, tasks, feeds } = cache

		// check if peer hasn't already closed their stuff in done-task-cleanup
		if (!sockets[remotestringkey]) return

		sockets[remotestringkey].count--

		log({ type: 'close stuff', data: { 
			text: 'close-stuff', 
			remotestringkey, 
			sockets: Object.keys(sockets),
			count: sockets[remotestringkey].count,
		}})
		
		// delete the connection
		const task_topics = Object.keys(tasks)
		const task_topics_len = task_topics.len

		for (var i = 0; i < task_topics_len; i++) {
			const stringtopic = task_topics[i]
			const remotekeys = Object.keys(tasks[stringtopic].connections)

			// delete stuff related to target peers
			if (targets[remotestringkey]) {				
				delete targets[remotestringkey].topics[stringtopic]
				const target_topics_len = Object.keys(targets[remotestringkey].topics).length
				if (!target_topics_len) {
					delete targets[remotestringkey]
					const string_msg = sockets[remotestringkey].channel.messages[0]
					string_msg.registry.delete(stringtopic)
				}
			}
			
			var open_tasks = 0
			Object.keys(tasks[stringtopic].roles).forEach(role => { open_tasks += tasks[stringtopic].roles[role] })

			// if job count for this remotestringkey is 0, close/end stuff
			if (sockets[remotestringkey] && !sockets[remotestringkey].count && !targets[remotestringkey]) {
				const { socket, count, replicationStream, channel, mux } = sockets[remotestringkey]
				// log({ type: 'close-stuff', data: { text: 'No more count for this socket', remotestringkey, count: sockets[remotestringkey].count }})
				// mux.unpair({ protocol: 'datdot/alpha' })
				if (channel) channel.close()
				mux.unpair({ protocol: 'datdot/alpha' })
				mux.destroy()
				socket.destroy()
				replicationStream.destroy()
				// and delete the socket from cache 
				delete tasks[stringtopic].connections[remotestringkey]
				delete cache.sockets[remotestringkey]
			}
			remotekeys.forEach(key => { 
				if (key === remotestringkey) {
					delete tasks[stringtopic].connections[key]
					// delete tasks if no jobs 
					if (!open_tasks) { 
						// log({ type: 'close-stuff', data: { text: 'leaving topic, no active tasks', stringtopic } })
						swarm.leave(b4a.from(stringtopic, 'hex')) // stop getting new connections on topic. leave will not close any existing connections.
						
						const feedkeys = Object.keys(feeds[stringtopic])
						feedkeys.forEach(async key => {
							const {feed} = feeds[stringtopic][key]
							await feed.core.close()
						})
		
						delete tasks[stringtopic]
						
					}
				}
			})
		}
		
	}


	
	/* --------------------------------------------------------------------------------------------
	
	CACHE
	
	-----------------------------------------------------------------------------------------------*/

	/* 
	account = {
		storage: {
				feedkey: { feed, db, discovery, unintercept }
		},
    cache:  {
        swarm,
        sockets: {
            [remotestringkey]: { 
								count: 1 // connection count
                socket, 
                replicationStream,
								mux,
								channel: {
									messages: {
										// different types, our only type for now is string_msg (channel.messages[0])
										registry: { msg_queue: [message], onmessage }
										onmessage: dispacher
									}
								},
            }
        },
				tasks: {
					[stringtopic]: {
						connections: {
							[remotestringkey]: true
						},
						roles: {
							author: 0, // { server: true }
							sponsor: 0, // { client: true } 
							encoder2author: 1, // { client: true }
							encoder2attestor: 0, // { server: true }
							attestor2encoder: 0, // { client: true }
							attestor2hoster: 0, // { server: true }
							hoster2author: 0, // { server: true, client: true }
							hoster2attestor: 0, // { client: true }
							hoster2peers: 0, // { server: true }
							perf_attestor: 0 
						}
					}
				}
				feeds: {
					[stringtopic]: {
						[stringkey]: { feed }
					}
				}
				// targets are created only when we have a msg (send or receive) and a targetList and we need to exchange keys first and then replicate
				targets: {  
					[remotestringkey]: {
						topics: {
							[stringtopic1]: { feed, msg: { receive, send }, ontarget, done, log },
							[stringtopic2]: { feed, msg: { receive, send }, ontarget, done, log }
						}
					}
				},
    },
} 

*/
