const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const RAM = require('random-access-memory')
const datdot_crypto = require('datdot-crypto')
const mux_proto = require('_datdot-service-helpers/mux-proto')
const b4a = require('b4a')
const c = require('compact-encoding')
const try_refresh_discovery = require('_datdot-service-helpers/try-refresh-discovery')
const safetyCatch = require('safety-catch')
const DEFAULT_TIMEOUT = 5000
const {
	done_task_cleanup,
	remove_from_roles,
	get_tasks_count_for_topic,
	close_feeds,
	remove_from_tasks,
	get_connections,
	remove_from_feeds,
	decrease_socket_count,
	remove_from_targets,
	get_socket_count,
	remove_from_sockets,
	close_streams_and_channels,
	get_target_tasks,
	remove_from_target_tasks,
	remove_from_task_connections,
	remove_from_registry,
	get_tasks
} = require('_datdot-service-helpers/done-task-cleanup')


module.exports = hyper

function hyper (account, bootstrap) {
	const api = {
		new_task: (opts) => _new_task(account, opts),
		connect: (opts) => _connect(account, bootstrap, opts),
	}
	return api
}
/* --------------------------------------------------------------------------------------------

	NEW TASK

-----------------------------------------------------------------------------------------------*/
async function _new_task(account, { newfeed = true, feedkey, topic, log }) { // newfeed false for receivers of feedkey (attester, hoster)
	return new Promise (async (resolve, reject) => {

    log({ type: 'hyper', data: { text: 'starting new task' }})
    var feed
    var stringkey
    var stringtopic
    if (!account.state) account.state = { sockets: {}, feeds: {}, targets: {}, tasks: {} }

    const feeds = account.state.feeds
    
    // 0. If feedkey and/or topic, get stringkey and stringtopic
    if (topic) { 
      topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
      stringtopic = topic.toString('hex') 
    }
    if (feedkey) { // when feed key with or without topic
      feedkey = Buffer.isBuffer(feedkey) ? feedkey : b4a.from(feedkey, 'hex')
      stringkey = feedkey.toString('hex')
      if (!topic) {
        topic = datdot_crypto.get_discoverykey(feedkey)
        stringtopic = topic.toString('hex')
      }
    } 
    
    // 1. Get or make feed
    if (stringtopic && stringkey && feeds[stringtopic]) { // get existing feed
      log({ type: 'hyper', data: { text: 'Existing feed', stringkey, stringtopic }})
      feed = feeds[stringtopic][stringkey].feed 
    } else { // make new feed
      if (newfeed) {  // attester and hoster have to wait to receive feedkey first, so no new feed for them
        if (feedkey) {
          // log({ type: 'hyper', data: { text: 'New feed', feedkey: feedkey.toString('hex') }})
          feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
          add_to_feeds({ feeds, stringtopic, stringkey, feed })
          await feed.ready().catch(safetyCatch)
        } else {
          feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
          // log({ type: 'hyper', data: { text: 'New feed', feedkey: 'none' }})
          await feed.ready().catch(safetyCatch)
          feedkey = feed.key
          stringkey = feedkey.toString('hex')
          if (!topic) topic = datdot_crypto.get_discoverykey(feedkey)
          if (!stringtopic) stringtopic = topic.toString('hex')
          add_to_feeds({ feeds, stringtopic, stringkey, feed })
        }
        log({ type: 'hyper', data: { text: 'New feed', stringkey, stringtopic }})
      }
    }

    resolve({ feed })
  })
}
/* --------------------------------------------------------------------------------------------

	CONNECT

-----------------------------------------------------------------------------------------------*/
async function _connect ( account, bootstrap, { swarm_opts, targets: Targets, log }) {
	return new Promise (async (resolve, reject) => {
    Targets = Object.assign({ targetList: [], ontarget: noop, done: noop, msg: {} }, Targets)
    const { targetList, ontarget, msg, feed, done } = Targets
    var { role, topic, mode } = swarm_opts
    const { noisePublicKey, noisePrivateKey, state } = account
    var { swarm, tasks, targets, sockets } = state
    var stringtopic
    
    // log({ type: 'connect', data: { text: 'load-store connect', msg }})

    if (!topic) throw new Error('Error: Can not connect without topic')
    topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
    stringtopic = topic.toString('hex') 

    if (role === 'storage_hoster' || role === 'storage_attester') {
      console.log({
        text: 'connect for storage challenge',
        name: log.path,
        role,
        targetList,
        tasks: Object.keys(account.state.tasks),
        targets: Object.keys(account.state.targets),
        feeds: Object.keys(account.state.feeds),
        conn: Object.keys(tasks).map(stringtopic => Object.keys(tasks[stringtopic].connections)), 
        sockets: Object.keys(account.state.sockets).map(key => [key, account.state.sockets[key].count]),
      })
    }

    // 2. Make swarm
    if (!swarm) {
      swarm = new hyperswarm({ 
        bootstrap, 
        keyPair: { publicKey: noisePublicKey, secretKey: noisePrivateKey }, 
        maxParallel: Infinity, 
        maxPeers: Infinity 
      })
      account.state.swarm = swarm
      log({ type: 'store connect', data: { text: 'new swarm', mode }})
      swarm.on('connection', onconnection(account, log))
    }


    // 3. Store tasks (for topic)
    add_to_tasks({ tasks, stringtopic, role, log })


    // 4. Join peer through existing channel or join the swarm on topic
    if (!targetList.length) { // Join topic (if no targetList && no discovery yet)
      if (!swarm.status(topic)) {
        log({ type: 'store connect', data: { text: 'Join swarm', mode, stringtopic }})
        await swarm.join(topic, mode).flushed()
      } else {
        log({ type: 'store connect', data: { text: 'try refresh discovery', mode, stringtopic }})
        mode = await try_refresh_discovery({ swarm, topic, tasks: tasks[stringtopic].roles, log })
      }
    } else { 
      // connect only to the targets on the list
      // topic created with derive-topic is not a valid topic (compressed so not unique anymore)
      // but we connect to targets on targetList directly so the topic is just here for trackings
      for (const remotestringkey of targetList) {
        add_to_targets({ targets, remotestringkey, stringtopic, ontarget, role, feed, msg, done, log })			
				// performance attester needs to trigger onconnection cb first, it can't use the existing connection right away
        // if (sockets[remotestringkey] && role !== 'performance_attester') {
        if (sockets[remotestringkey]) {
					const { tid, channel } = sockets[remotestringkey]
					clearTimeout(tid)
					// if (!channel.opened) channel.open()
          log({ type: 'store connect', data: { text: 'Target - existing socket', connections_count: sockets[remotestringkey].count, channel: channel.opened }})
					handle_target_connection({ account, remotestringkey, log }) // feed will be returned with ontarget callback
        } else {				
          // Join peer (if no socket connection && no discovery yet)
          log({ type: 'store connect', data: { text: 'Target - joinPeer', remotestringkey, stringtopic, mode }})
          // await swarm.join(topic, mode).flushed()
          await swarm.joinPeer(b4a.from(remotestringkey, 'hex'))
          await swarm.listen() // explicitly start listening for incoming connections
        }
      }
    }
    resolve()
  })
}
/* --------------------------------------------------------------------------------------------

	ONCONNECTION

-----------------------------------------------------------------------------------------------*/
function onconnection (account, conn_log) {
	return async (connection, peerInfo) => {
		try {
			const { state } = account
			const { targets, tasks, sockets, swarm } = state
			const remotestringkey = peerInfo.publicKey.toString('hex')	
			const conn_log = account.log.sub(`<-onconnection: me: ${account.noisePublicKey.toString('hex').substring(0,5)}, peer: ${remotestringkey.substring(0,5)} `)
			conn_log({ type: 'onconnection', data: { text: 'onconnection callback', remotestringkey, /* isInitiator: connection.isInitiator */ } })

			// return if peer is trying to connect while new_channel (load_replicationStream) is still in progress
			// @TODO: we are trying to fix problem where one peer gets onclose and the other doesn't
			// the one that doesn't before the timeout gets a new onconnection 
			// and tries to reuse the existing channel, while the other peer cleaned the old channel
			// and is on new onconnection creating a new channel
			// we need to return until setTimeout removes old channel
			// if (sockets[remotestringkey] && !sockets[remotestringkey].channel.opened) return

      const tid = setTimeout(() => {
        conn_log({ type: 'onconnection', data: { text: 'ghost connection timeout', remotestringkey }})
        // we check for count to see if the connection was a match for any of the tasks (count would then not be a zero)
				// if there was no match or process stopped before the replication, then we close the streams
				if (sockets[remotestringkey] && !sockets[remotestringkey].count) {
          close_streams_and_channels({ sockets, remotestringkey, log: conn_log })
          remove_from_sockets({ sockets, remotestringkey, log: conn_log })
          conn_log({ type: 'onconnection', data: { text: 'ghost connection removed', remotestringkey }})
        }
      }, DEFAULT_TIMEOUT)
			
			if (!sockets[remotestringkey]) await load_replicationStream({ account, tid, connection, remotestringkey, log: conn_log })

			// this is the target
			if (targets[remotestringkey]) return handle_target_connection({ account, remotestringkey, log: conn_log }) 

			// peer is a client
			const peer_topics = peerInfo.topics
			if (!peer_topics.length) return 
			clearTimeout(tid)
			for (const topic of peer_topics) {
				const stringtopic = topic.toString('hex') // only shows topics derived from a feed.discoveryKey
				if (
          !tasks[stringtopic] || 
          tasks[stringtopic]?.connections[remotestringkey]
        ) continue
				handle_matching_topic({ stringtopic, remotestringkey, account, log: conn_log })
			}
		} catch (err) {
			conn_log({ type: 'Error', data: { text: 'Error: in onconnection', err }})
		}
	}
}		

async function load_replicationStream ({ account, tid, connection, remotestringkey, log }) {
	log({ type: 'onconnection', data: { text: 'loading replication stream and channel', remotestringkey } })
	try {
		const { tasks, sockets, targets } = account.state
		var replicationStream
		log({ type: 'onconnection', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
		replicationStream = hypercore.createProtocolStream(connection, { ondiscoverykey })
		mux = hypercore.getProtocolMuxer(replicationStream)
		add_to_sockets({ sockets, remotestringkey, connection, replicationStream, mux, tid, log })
		add_stream_listeners({ state: account.state, remotestringkey, log })
		
		const channel = await new_channel({ account, remotestringkey, log })
		if (!channel) return log({ type: 'exchange-feedkey', data: { text: 'Error: no channel returned', remotestringkey, existing: sockets[remotestringkey].channel ? 'yes' : 'no' }})

		return { replicationStream }

		// peer is a server
		function ondiscoverykey (discoverykey) { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
      const stringtopic = discoverykey.toString('hex') // only shows topics derived from a feedn.discoveryKey
			if (targets[remotestringkey] || !tasks[stringtopic] || tasks[stringtopic]?.connections[remotestringkey]) return
			clearTimeout(tid)
			handle_matching_topic({ stringtopic, remotestringkey, account, log })
		}
	} catch (err) {
		log({ type: 'Error', data: { text: 'Error: in loading replication stream and channel', err }})
	}
}

function handle_matching_topic ({ stringtopic, remotestringkey, account, log }) {
	const { state } = account
	const { sockets, tasks } = state
	const { channel, replicationStream } = sockets[remotestringkey]
	increase_socket_count({ sockets, remotestringkey, log })
	add_to_connections({ connections: tasks[stringtopic].connections, remotestringkey })
	log({ type: 'onconnection', data: { text: 'matching topic', remotestringkey, stringtopic } })
	replicate_feeds({ state, stringtopic, remotestringkey, replicationStream, log })

	const string_msg = channel.messages[0]
	var registry = string_msg.registry
	const entry = registry.get(stringtopic)				
	log({ type: 'handle target connection', data: { text: `registry entry in handle matching topic`, stringtopic, remotestringkey, entry: entry ? 'yes' : 'no'   } })
	if (!entry) {
		string_msg.registry.set(stringtopic, { onmessage })
	} else {
		const { msg_queue } = entry
		if (msg_queue) msg_queue.forEach(message => onmessage({ account, message, remotestringkey, string_msg, state }))
		entry.msg_queue = void 0
		entry.onmessage = onmessage
	}
}

function replicate_feeds ({ state, stringtopic, remotestringkey, replicationStream, log }) {
	try {
    const { swarm, feeds, sockets } = state
		const feedkeys = Object.keys(feeds[stringtopic])
		const status = swarm.status(b4a.from(stringtopic, 'hex'))
		if (!status) return		
		for (const key of feedkeys) {
			const feed = feeds[stringtopic][key].feed
			if (!feed.opened || feed.closing) {
				log({ type: 'replicate', data: { text: 'error: feed closing or closed', open: feed.opened, closing: feed.closing, stringtopic, feedkey: feed.key.toString('hex') } })
				continue
			}
			log({ type: 'replicate', data: { text: 'replicate feed', stringtopic, feedkey: feed.key.toString('hex') } })
			feed.replicate(replicationStream)
		}
	} catch (err) {
		log({ type: 'Error', data: { text: 'Error: replicating feeds in the general swarm', err }})
	}
}

async function handle_target_connection ({ account, remotestringkey, log }) { //get feedkey from or send to a target peer (on each custom topic)
	const { state } = account
	const { targets, sockets, tasks } = state
	log({ type: 'onconnection', data: { text: `handle target conn`, remotestringkey, target_tasks: Object.keys(targets[remotestringkey].tasks).length } })				
	
	
	try {		
		const { tid, channel, replicationStream } = sockets[remotestringkey]
		clearTimeout(tid)
		log({ type: 'exchange-feedkey', data: { text: 'load channel', remotestringkey, opened: channel.opened }})
		
		// load registry
		const string_msg = channel.messages[0] // get string message type for sending strings through the mux channel with that peer (channel.addMessage(...))
		var registry = string_msg.registry
		
		// we need to loop over all the target tasks to find tasks for this peer
		const target_tasks = Object.keys(targets[remotestringkey].tasks)
		for (const stringtopic of target_tasks) {
			try {	
				const { msg, role, feed, ontarget, log } = targets[remotestringkey].tasks[stringtopic]
				const task = tasks[stringtopic]
				if (!task) continue // task might have been cleaned by another process in the meanwhile
				const connections = task.connections
				
				log({ type: 'handle target connection', data: { text: `current peer ${remotestringkey} for topic ${stringtopic}`, target_tasks  } })
				
				// this task is already in progress
				// check for socket too because in case of 2 failed attempts in new_channel onclose, 
				// we will have connections[remotestringkey)] but we will remove sockets[remotestringkey] 
				// so we can create new replicationStream when new connection between same peers is established
				if (connections[remotestringkey] && sockets[remotestringkey]) {  
					log({ type: 'exchange-feedkey', data: { text: 'this task already in progress', remotestringkey }})
					continue
				}

				increase_socket_count({ sockets, remotestringkey, log })
				add_to_connections({ connections, remotestringkey })
	
				// check if any messagges are waiting in the queue
				const entry = registry.get(stringtopic)				
				log({ type: 'handle target connection', data: { text: `registry entry in target conn`, stringtopic, remotestringkey, entry: entry ? 'yes' : 'no'   } })
				if (!entry) {
					string_msg.registry.set(stringtopic, { onmessage })
				} else {
					const { msg_queue } = entry
					if (msg_queue) msg_queue.forEach(message => onmessage({ account, message, remotestringkey, string_msg, state }))
					entry.msg_queue = void 0
					entry.onmessage = onmessage
				}
				if (role === 'performance_attester') {
					replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
				}
				if (msg.send) { // see if we need to send or to receive a message
					log({ type: 'protomux', data: { text: `send feedkey` } })
					string_msg.send(JSON.stringify({ type: 'feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
				} 
			} catch (err) {
				log({ type: 'error', data: { text: 'Error: handle target connection', err, remotestringkey, stringtopic  } })
				// console.log({ targets: Object.keys(targets), tasks: Object.keys(tasks), sockets: Object.keys(sockets)})
			}
		}
	} catch (err) {
		return log({ type: 'exchange-feedkey', data: { text: 'Error: caught in handle target connection', remotestringkey, err }})
	}

}

function new_channel ({ account, remotestringkey, log }) {
	log({ type: 'protomux', data: { text: `new channel`, remotestringkey } })
	return new Promise(async (resolve, reject) => {
		try {
			const { state } = account
			const { sockets } = state
			var counter = 0
			const opts = { protocol: 'datdot/alpha' }
			var { mux, channel, replicationStream } = sockets[remotestringkey]
			
			if (channel && channel.opened) resolve(channel) // reuse existing opened channel

			mux_proto.make_protocol({ mux, opts, cb, log })
			
			async function cb () {
				counter++
				log({ type: 'protomux', data: { text: 'mux pair cb', remotestringkey, counter, is_channel: channel ? 'yes': 'no', op: channel?.opened, mux_opened: mux.opened(opts) }}) 
				// if (mux.opened(opts)) return reject()
				if (mux.opened(opts)) return

				if (channel && channel.opened) return resolve(channel)
				// else if (channel) channel.close() // close and then repeat the create_and_open_channel, so don't return

				channel = mux_proto.create_and_open_channel({ mux, opts: {...opts, onopen, onclose }, log })
				if (!channel) {
					log({ type: 'protomux', data: { text: 'Error: mux_proto returned no channel' }}) 
					return
				}
				sockets[remotestringkey].channel = channel				

				// add message types for different encodings (i.e. string...) + intercept onmessage with a dispacher
				var string_msg = channel.messages[0] 
				if (!string_msg) string_msg = channel.addMessage({ encoding: c.string, onmessage: dispacher })
				
				// load registry to store onmessage callbacks and undelivered messages (msg_queue) for each stringtopic
				var registry = string_msg.registry
				if (!registry) registry = string_msg.registry = new Map()

				// dispacher stores the message in the queue until we can process it
				// example: chain emits the event bob receives it and sends the message, but alice hasn't received the event yet
				function dispacher (message) {
					const { stringtopic } = JSON.parse(message)
					const entry = registry.get(stringtopic)
					log({ type: 'dispacher', data: { text: 'dispacher received', message, entry: entry ? 'yes' : 'no' }}) 
					if (!entry) {
						return registry.set(stringtopic, { msg_queue: [message] })		
					} 
					const { onmessage, msg_queue } = entry
					onmessage ? onmessage({ account, message, remotestringkey, string_msg, state }) : msg_queue.push(message)
				}
			}	

			function onopen () { 
				log({ type: 'protomux', data: { text: 'onopen: either side opened channel', counter_onopen: counter, mux_opened: mux.opened({ protocol: 'datdot/alpha' }) }}) 
				counter = 0 // we need to reset it so that it catches the 2 consecutive fails in onclose
				return resolve(channel)
			}
			async function onclose (isRemote, channel) { 
				log({ type: 'protomux', data: { text: 'onclose: either side closed channel', counter_onclose: counter, opened: channel.opened, closed: channel.closed }})
				channel.close() // notify other side, maybe they closed but are unaware
				// we don't reject here, because we need to wait for the second cb in this case
			}

		} catch (err) {
			log({ type: 'protomux', data: { text: `Error: new channel`, remotestringkey } })
			return reject(err)
		}
	})
}

async function onmessage ({ account, message, remotestringkey, string_msg, state }) {
	const { type, challenge_id, hosterkey, feedkey, stringtopic, proof_of_contact } = JSON.parse(message)
	// console.log({messssage: message })
	const { feeds, targets, sockets } = state
	const { replicationStream } = sockets[remotestringkey]
	const { ontarget, role, done, log } = targets[remotestringkey]?.tasks[stringtopic] || { log: account.log.sub(`performance hoster, challenge: ${challenge_id}`) }
	// log({ type: 'onmessage', data: { text: 'new message received', role, type } })

	var feed
	if (type === 'feedkey') {	// make feed
		log({ type: 'onmessage protomux', data: { text: 'received feedkey', feedkey: feedkey.toString('hex') }})
		feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
    await feed.ready().catch(safetyCatch)
		const stringkey = feedkey.toString('hex')
		add_to_feeds({ feeds, stringtopic, stringkey, feed })
		string_msg.send(JSON.stringify({ type: 'ack-feedkey', stringkey, stringtopic }))
		replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
		if (role === 'attester2encoder') set_target_task_status({ targets, remotestringkey, stringtopic, status: 'done' })
	} 
	else if (type === 'ack-feedkey') {
		feed = targets[remotestringkey].tasks[stringtopic].feed
		log({ type: 'onmessage protomux', data: { text: 'received ack', feedkey: feed.key.toString('hex') }})
		replicate_to_target({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log })
		if (role === 'encoder2attester') set_target_task_status({ targets, remotestringkey, stringtopic, status: 'done' })
	}
	else if (type === 'requesting-proof-of-contact') {
		// send proof
		const data = b4a.from(challenge_id.toString(), 'binary')
		const proof_of_contact = account.sign(data)
		log({ type: 'onmessage', data: { text: 'type: requesting-proof-of-contact', challenge_id, message } })
		const hosterkey = account.identity.noiseKey.toString('hex')
		string_msg.send(JSON.stringify({ type: 'proof-of-contact', stringtopic, hosterkey, proof_of_contact: proof_of_contact.toString('hex') }))
	}
	else if (type === 'proof-of-contact') {
	  log({ type: 'proof of contact', data: { proof_of_contact, hosterkey } })
		string_msg.send(JSON.stringify({ type: 'ack-proof-of-contact', stringtopic }))
		set_target_task_status({ targets, remotestringkey, stringtopic, status: 'done' })
		done(proof_of_contact, hosterkey)
	}
	else if (type === 'ack-proof-of-contact') {
		log({ type: 'ack-proof-of-contact', data: { stringtopic } })
		if (targets[remotestringkey]) {
			set_target_task_status({ targets, remotestringkey, stringtopic, status: 'done' })
			done()
		}
	}
}		

function replicate_to_target ({ state, ontarget, feed, replicationStream, remotestringkey, stringtopic, log }) {
	if (!feed.opened || feed.closing) return
	feed.replicate(replicationStream)
	set_target_task_status({ targets: state.targets, remotestringkey, stringtopic, status: 'pending' })
	log({ type: 'replicate', data: { text: 'replicate to target', stringtopic  }})
	ontarget({ feed, remotestringkey })
}
/*-----------------------------------------------------------------------------------------------------
		
	HELPERS
		
------------------------------------------------------------------------------------------------------*/
function add_to_tasks ({ tasks, stringtopic, role, log }) {
	if (!tasks[stringtopic]) {
		tasks[stringtopic] = { roles: { [`${role}`]: 1 }, connections: {} } 
		log({ type: 'hyper', data: { text: 'New topic', role, stringtopic }})
	} else {
		tasks[stringtopic].roles[role] ? tasks[stringtopic].roles[role]++ : tasks[stringtopic].roles[role] = 1
		log({ type: 'hyper', data: { text: 'Existing topic', role, stringtopic, tasks: tasks[stringtopic] }})
	}
}

function set_target_task_status ({ targets, remotestringkey, stringtopic, status }) {
	targets[remotestringkey].tasks[stringtopic].status = status
}

function add_to_targets ({ targets, remotestringkey, stringtopic, role, ontarget, feed, msg, done, log }) {
	if (!targets[remotestringkey]) targets[remotestringkey] = { tasks: { [`${stringtopic}`]: { ontarget, role, feed, msg, done, log } } }
	else targets[remotestringkey].tasks[stringtopic] = { ontarget, role, feed, msg, done, log }
}
		
function add_to_sockets ({ sockets, remotestringkey, connection, replicationStream, mux, tid, log }) {
	log({ type: 'hyper', data: { text: 'Adding to sockets', remotestringkey, does_exist: sockets[remotestringkey] ? 'yes' : 'no' }})
	sockets[remotestringkey] = { socket: connection, replicationStream, tid, mux }
  // count is undefined and will be set to 1 once there is a task match
}

function increase_socket_count ({ sockets, remotestringkey, log}) {
	sockets[remotestringkey].count ? sockets[remotestringkey].count++ : sockets[remotestringkey].count = 1
	log({ type: 'socket', data: { text: 'Increase count', remotestringkey, count: sockets[remotestringkey].count   }})
}

function add_to_connections ({ connections, remotestringkey }) {
  connections[remotestringkey] = true
}

function add_to_feeds ({ feeds, stringtopic, stringkey, feed }) {
	if (!feeds[stringtopic]) feeds[stringtopic] = {}
	if (!feed) return
	feeds[stringtopic] = { [stringkey]: { feed } }
}

function add_stream_listeners ({ state, remotestringkey, log }) {
	const { sockets } = state
	const { replicationStream } = sockets[remotestringkey]

	replicationStream.on('error', (err) => { 
		// log({ type: 'replicationStream on', data: { text: `on error: replicationStream error`, err, code: err.code, stack: err.stack /*snapshot: get_snapshot(account, remotestringkey)*/ } })
	})

	replicationStream.on('close', () => { 
		log({ type: 'replicationStream on', data: { text: `on close: replicationStream closed`, remotestringkey, socket_exist: sockets[remotestringkey] ? 'yes' : 'no', count: sockets[remotestringkey]?.count } }) 
		if (sockets[remotestringkey]) {
			const { count } = sockets[remotestringkey]
			// error occured immediatelly after onconnection & loading of the streams
			if (!count) {
				close_streams_and_channels({ sockets, remotestringkey, log })
				remove_from_sockets({ sockets, remotestringkey, log })
				return
			}

			// cleanup for connections with and without the channel (targets and general swarm)
			handle_done_task_cleanup({ state, remotestringkey, log })
		}
	})
	replicationStream.on('data', (data) => { 
		// log({ type: 'replicationStream on', data: { text: `on data: replicationStream`, data, stack: err.stack } }) 
	})
}

async function handle_done_task_cleanup ({ state, remotestringkey, log }) {
	const { swarm, sockets, targets, tasks, feeds } = state
	
	const all_topics = get_tasks({ tasks, log })

	log({ type: 'on close', data: { 
		text: `handling on close event`, 
		remotestringkey,
		all_topics: all_topics,
		roles: all_topics.map(stringtopic => tasks[stringtopic].roles),
		conn: all_topics.map(stringtopic => Object.keys(tasks[stringtopic].connections)), 
		sockets: Object.keys(sockets).map(key => [ key, sockets[key].count ])
	} }) 
	
	for (const stringtopic of all_topics) {
		const topic = b4a.from(stringtopic, 'hex')
		const task = tasks[stringtopic]
		if (!task) continue // task might have been cleaned by another process in the meanwhile
		if (task.connections[remotestringkey]) {
			// --------- TARGET ------------
			// stringtopic is one time used & is unique 
			if (task.roles['encoder2attester'] || task.roles['attester2hoster'] || task.roles['storage_hoster']) {
				// TASKS
				remove_from_tasks({ tasks, stringtopic, log })
				// TARGETS
				remove_from_target_tasks({ targets, stringtopic, remotestringkey, log }) // we have always only one task per stringtopic for targets
				const target_tasks = get_target_tasks({ targets, remotestringkey, log })
				if (!target_tasks.length) {
					log({ type: 'handle cleanup', data: { text: 'remove from targets, remove registry', role, remotestringkey, stringtopic  }})
					remove_from_targets({ targets, remotestringkey, log })
					remove_from_registry({ sockets, stringtopic, remotestringkey, log })
				}
				// FEEDS
				close_feeds({ feeds, stringtopic, log })
				//SWARM
        swarm.leavePeer(b4a.from(remotestringkey, 'hex')) // stop attempting direct connections to the peer
				// SOCKETS
				decrease_socket_count({ sockets, remotestringkey, log })
				const connections_for_socket = get_socket_count({ sockets, remotestringkey, log })
				if (!connections_for_socket) {
					close_streams_and_channels({ sockets, remotestringkey, log })
					remove_from_sockets({ sockets, remotestringkey, log })
				}
			}
			// --------- NO TARGET/GENERAL SWARM -------------
			else if (task.roles['author'] || task.roles['hoster']) {
				// TASKS
				var role 
				if (task.roles['author']) role = 'author'
				else if (task.roles['hoster']) role = 'hoster'
				remove_from_roles({ tasks, role, stringtopic, log })
				decrease_socket_count({ sockets, remotestringkey, log })
				const connections_for_socket = get_socket_count({ sockets, remotestringkey, log })
				if (!connections_for_socket) {
					close_streams_and_channels({ sockets, remotestringkey, log })
					remove_from_task_connections({ tasks, stringtopic, remotestringkey, log })
					remove_from_sockets({ sockets, remotestringkey, log })
				}
				const topic_tasks = get_tasks_count_for_topic({ tasks, stringtopic, log })
				if (!topic_tasks) {
					remove_from_tasks({ tasks, stringtopic, log })
					close_feeds({ feeds, stringtopic, log })
					swarm.leave(topic) // stop getting new connections on topic. leave will not close any existing connections.
				} 
				else await try_refresh_discovery({ swarm, topic, tasks: tasks[stringtopic].roles, log }) 
			}
		}
    log({ type: 'on close', data: { 
      text: `handling onclose finished`, 
      remotestringkey,
      stringtopic,
      target: targets[remotestringkey] ? 'yes' : 'no',
      all_topics: get_tasks({ tasks, log }),
      conn: Object.keys(tasks).map(stringtopic => Object.keys(tasks[stringtopic].connections)), 
      sockets: Object.keys(sockets).map(key => [ key, sockets[key].count ])
    } }) 
	}

}

function noop () {}

/* --------------------------------------------------------------------------------------------

	state

-----------------------------------------------------------------------------------------------*/

/* 
account = {
	storage: {
			feedkey: { feed, db, discovery, unintercept }
	},
	state:  {
			swarm,
			sockets: {
					[remotestringkey]: { 
							count: 1 // connection count (when connection is established, for every task with this peer)
							socket, 
							replicationStream,
							mux,
							channel: {
								messages: {
									// different types, our only type for now is string_msg (channel.messages[0])
									registry: { msg_queue: [message], onmessage }
									onmessage: dispacher
								}
							},
					}
			},
			tasks: {
				[stringtopic]: {
					connections: {
						[remotestringkey]: true 
					},
					roles: {
						author: 0, // { server: true }
						sponsor: 0, // { client: true } 
						encoder2author: 1, // { client: true }
						encoder2attester: 0, // { server: true }
						attester2encoder: 0, // { client: true }
						attester2hoster: 0, // { server: true }
						hoster2author: 0, // { server: true, client: true }
						hoster2attester: 0, // { client: true }
						hoster: 0, // { server: true }
						performance_attester: 0, { client: true }
						storage_attester: 0 { client: true }
					}
				}
			}
			feeds: {
				[stringtopic]: {
					[stringkey]: { feed }
				}
			}
			targets: {  
				[remotestringkey]: { // rename to stringremotekey
					tasks: {
						[stringtopic1]: { feed, msg: { receive, send }, ontarget, role, done, status, log }, // status set when replication starts
						[stringtopic2]: { feed, msg: { receive, send }, ontarget, role, done, status, log }
					}
				}
			},
			// targets are created only when we have a msg (send or receive) and a targetList and we need to exchange keys first and then replicate
	},
} 

*/