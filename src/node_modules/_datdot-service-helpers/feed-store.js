const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const NoiseSecretStream = require('@hyperswarm/secret-stream')
const HypercoreProtocol = require('hypercore-protocol')
const RAM = require('random-access-memory')
const register_extension = require('_datdot-service-helpers/register-extension')
const ready = require('_datdot-service-helpers/hypercore-ready')
const datdot_crypto = require('../datdot-crypto')
const FeedStorage = require('_datdot-service-helpers/feed-storage.js')
const sub = require('subleveldown')
const b4a = require('b4a')

module.exports = feed_store

function feed_store (account) {
	const api = {
		load_feed: (opts) => _load_feed(account, opts),
		no_tasks: (opts) => _no_tasks(account, opts)
	}
	return api
}

function noop () {}

async function _load_feed(account, { extension = {}, swarm_opts, peers, msg = {}, feedkey, log }) {
	var { topic, mode } =  swarm_opts
	const { ext_cbs, name } = extension
	peers = Object.assign({ peerList: [], onpeer: noop }, peers)
	const { peerList, onpeer } = peers
	const { send, receive } = msg

	log({ type: 'feed-store', data: { text: 'loading feed', peerList: peerList.length, msg, extension, swarm_opts, peers, feedkey }})
	
	var feed
	var stringkey
	var swarm
	var stringtopic
	var storage
	
	if (!account.cache) account.cache = { sockets: {}, topics: {}, tasks: {} }
	const topics = account.cache.topics

	if (feedkey) { // when feed key with or without topic & no msg.recieve
		if (!topic) topic = datdot_crypto.get_discoverykey(feedkey)
		else topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
		stringkey = feedkey.toString('hex')
		stringtopic = topic.toString('hex')
	} else if (topic) { // when msg.receive (no feedkey, only topic)
		topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
		stringtopic = topic.toString('hex') 
	}
	
	// 1. Get or make feed
	if (stringtopic && topics[stringtopic]?.feeds[stringkey]) { // get existing feed
		feed = topics[stringtopic].feeds[stringkey].feed 
		log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
	} else { // make new feed
		if (!msg.receive) { 
			// 1. Get or make feed
			if (feedkey) {
				feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
			} else {
				feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
				await feed.ready()
				feedkey = feed.key
				stringkey = feedkey.toString('hex')
				topic = datdot_crypto.get_discoverykey(feedkey)
				stringtopic = topic.toString('hex')
			}
			log({ type: 'feed-store', data: { text: 'New feed', stringkey, stringtopic }})
		}
		// 2. Store topic and feed

		if (!topics[stringtopic]) topics[stringtopic] = { feeds: {}, sockets: {} } // add topic to the cache
		if (!msg.receivetopics && !topics[stringtopic].feeds[stringkey]) topics[stringtopic].feeds[stringkey] = { feed }
		log({ type: 'feed-store', data: { text: 'New topic stored in cache', stringtopic }})
	}
	// 3. Make swarm
	if (!account.cache.swarm) {
		swarm = new hyperswarm()
		account.cache.swarm = swarm
		log({ type: 'feed-store', data: { text: 'new swarm', mode }})
		swarm.on('connection', onconnection(account, log))
	} else swarm = account.cache.swarm
	// 4. Join topic (if no discovery yet)
	var discovery = topics[stringtopic].discovery
	if (!discovery) {
		await swarm.join(topic, mode).flushed()
		topics[stringtopic].discovery = swarm.status(topic)
	}
	log({ type: 'feed-store', data: { text: 'swarm joined the swarm', mode, stringtopic }})

	// 5. Add task
	const tasks = account.cache.tasks
	log({ type: 'feed-store', data: { text: 'tasks before adding latest', tasks, stringtopic }})
	if (!tasks[stringtopic]) tasks[stringtopic] = 1
	else tasks[stringtopic]++
	log({ type: 'feed-store', data: { text: 'tasks after adding latest', tasks, stringtopic }})

	log({ type: 'feed-store', data: { text: 'returning feed', stringkey }})
	if (!msg.receive) return { feed }

	function onconnection (account, log) {
		const sockets = account.cache.sockets
		var replicationStream
		
		return (connection, info) => {
			log({ type: 'onconnection', data: { text: 'connection callback' } })
			const remotekey = connection.remotePublicKey
			const remotestringkey = remotekey.toString('hex')
			if (sockets[remotestringkey]) {
					log({ type: 'feed-store', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
					replicationStream = sockets[remotestringkey].replicationStream
			} else {
					log({ type: 'feed-store', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
					replicationStream = hypercore.createProtocolStream(connection, { ondiscoverykey })
					sockets[remotestringkey] = { socket: connection, replicationStream }
			}

			// log({ type: 'swarm', data: { text: `New connection`, socket: socket.isInitiator  } })
			// next({ ext, feed, remotekey, log })

			/*
			1. hoster, downloading data from author
			2. feed is stored in cache and storage
			3. 
			*/
			
			if (info.topics.length) {
				if (peerList.length > 0) return is_on_peerList()
				for (var i = 0, len = info.topics.length; i < len; i++) {
					const Topic = info.topics[i].toString('hex')
					log({ type: 'feed-store', data: { text: 'topics', Topic } })
					if (topics[Topic]) {
						var topics_sockets = topics[Topic].sockets
						if (!topics_sockets[[remotestringkey]]) topics[Topic].sockets[remotestringkey] = sockets[remotestringkey]
						Object.keys(topics[Topic].feeds).forEach(key => {
							const feed = topics[Topic].feeds[key].feed
							log({ type: 'onconnection', data: { text: 'matching topic - client', peerList, remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
							feed.replicate(replicationStream)
						}) 
					}
				}
			} 

			function ondiscoverykey (discoverykey) { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
				if (peerList.length > 0) return is_on_peerList()
				if (info.topics.length) return
				log({ type: 'onconnection', data: { text: 'on discovery key' } })
				const Topic = discoverykey.toString('hex')
				if (topics[Topic]) { 
					var topics_sockets = topics[Topic].sockets
					if (!topics_sockets[[remotestringkey]]) topics[Topic].sockets[remotestringkey] = sockets[remotestringkey]
					Object.keys(topics[Topic].feeds).forEach(key => {
						const feed = topics[Topic].feeds[key].feed
						log({ type: 'onconnection', data: { text: 'matching topic - server', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
						feed.replicate(replicationStream)
					}) 
				}
			}
			function is_on_peerList () {
				log({ type: 'feed-store', data: { text: `is on peer list`, peer: peerList[0].toString('hex')  } })
				var p = Buffer.isBuffer(peerList[0]) ? peerList[0] : b4a.from(peerList[0], 'hex') // maybe connection.remoteKey
				if (msg) { // if peer need to send/receive feedkey first
					const { send, receive } = msg
					if (send) {
						log({ type: 'feed-store', data: { text: `sending the message` } })
						replicationStream.write((JSON.stringify({ type: 'feedkey', feedkey: feed.key.toString('hex') })))
						log({ type: 'feed-store', data: { text: 'send msg - feedkey', feedkey: feed.key.toString('hex') }})
						feed.replicate(replicationStream) // replicate if you don't need to receive for the feedkey
						swarm.leavePeer(p)
						return onpeer({ targetkey, stringkey })
					}
					else if (receive) {
						log({ type: 'feed-store', data: { text: `receiving the message` } })
						replicationStream.on('data', async (message) => {
							try {
								const { type, feedkey } = JSON.parse(message.toString('utf-8'))
								log({ type: 'feed-store', data: { text: 'receive msg - feedkey', feedkey: feedkey.toString('hex') }})
								if (type === 'feedkey') {
									// 1. Make feed
									feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
									await feed.ready()
									feedkey = feed.key
									stringkey = feedkey.toString('hex')
									topics[stringtopic].feeds[stringkey] = { feed }
									log({ type: 'feed-store', data: { text: 'New feed - fresh', stringkey, stringtopic }})
									feed.replicate(replicationStream) // replicate once you have the feed
									swarm.leavePeer(p)
									return onpeer({ feed, next, targetkey, stringkey })
								}
							} catch (err) {
								log({ type: 'error', data: { text: 'Error: is on peerList', err } })
							}
						})
					}
				}
			}
			
			replicationStream.on('close', (err) => { log({ type: 'feed-store', data: { text: `Error: replicationStream closed` } }) })
			replicationStream.on('end', (err) => { log({ type: 'feed-store', data: { text: `Error: replicationStream ended` } }) })
			replicationStream.on('timeout', (err) => { log({ type: 'feed-store', data: { text: `Error: replicationStream timed out` } }) })
		}
	}
}

async function _no_tasks (account, opts) {
	const { stringtopic, log } = opts
	log({ type: 'no open tasks', data: { text: 'end streams, leave swarm, close feeds, destroy discovery', stringtopic } })
	const { swarm, sockets, topics, tasks } = account.cache 
	delete tasks[stringtopic]
	const remotekeys = Object.keys(topics[stringtopic].sockets)
	// close all the streams
	remotekeys.forEach(key => {
		const { socket, replicationStream } = sockets[key]
		// if socket not in use, close it
		socket.end()
		replicationStream.end()
		// delete the socket object
		delete sockets[key]
		delete account.cache.sockets[key]
	})
	// swarm leave topic
	swarm.leave(b4a.from(stringtopic, 'hex'))
	// close all the feeds
	const feedkeys = Object.keys(topics[stringtopic].feeds)
	feedkeys.forEach(async key => {
		const {feed} = topics[stringtopic].feeds[key]
		await feed.core.close()
	})
	// destroy the discovery
	// topics[stringtopic].discovery.leave()
	const discovery = await topics[stringtopic].discovery
	if (discovery) await discovery.destroy()
	// delete the topic object
	delete topics[stringtopic]
}


////////////////////////////////////////



////////////////////////////////////////

/* 
account = {
    cache:  {
        swarm,
        sockets: {
            remotekey: { 
                socket, 
                replicationStream
            }
        },
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { feed }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        },
        tasks: {
            stringkey: counter
        }
    },
    storage: {
        feedkey: { feed, db, discovery, unintercept }
    }
} 







 SCENARIOS

 cache: {
     'persist': {
        swarm, // autogenerated keypair
        sockets,
        topics,
        tasks
     },
     'fresh': {
         '1': {
            swarm,
            sockets,
            topics,
            tasks    
        }
    },
     'intercept': {
        swarm, // noise keypair, stored on chain
        sockets,
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { 
                        feed,
                        storage
                    }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        },
        tasks
     }
 }

 ----
 ENCODING

 - starting

* feed !fresh
* can reuse a feed from other !fresh (sponsor)
* if no feed yet, make new
* if no swarm, create one (store it in cache under swarm pubkey)

 - finishing

* when feed not in use, close
* when swarm not in use, close
* when topic not in use, leave


 ----
 ATTESTING - performance challenge

  - starting

 * feed is fresh

 - finishing


 ----
 HOSTING

 - starting

 * feed is intercepted && !fresh


 - finishing

 ----
 AUTHOR

  - starting

* feed is fresh


 - finishing


 ----
 SPONSOR

 - starting

 * feed !fresh

 - finishing

---

*/

