const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const HypercoreProtocol = require('hypercore-protocol')
const RAM = require('random-access-memory')
const register_extension = require('_datdot-service-helpers/register-extension')
const ready = require('_datdot-service-helpers/hypercore-ready')
const datdot_crypto = require('../datdot-crypto')
const FeedStorage = require('_datdot-service-helpers/feed-storage.js')
const sub = require('subleveldown')
var pump = require('pump')

module.exports = feed_store

function feed_store (account) {
    const api = {
        load_feed: (opts) => _load_feed(account, opts),
        no_tasks: (...args) => _no_tasks(account, ...args)
    }
    return api
}

function noop () {}

async function _load_feed(account, { config, extension = {}, swarm_opts, peers, feedkey, log }) {
    const { intercept, fresh, persist } = config // feed config
    var { topic, mode } =  swarm_opts
    const { ext_cbs, name } = extension
    peers = Object.assign({ peerList: [], onpeer: noop }, peers)
    const { peerList, onpeer } = peers
    
    log({ type: 'feed-store', data: { text: 'loading feed', config, extension, swarm_opts, peers, feedkey }})
    
    var feed
    var feedkey
    var stringkey
    var swarm
    var stringtopic
    var storage
    const cache = account.cache
    const next = Object.keys(cache['fresh']).length + 1
    
    if (feedkey) {
        topic = datdot_crypto.get_discoverykey(feedkey)
        stringkey = feedkey.toString('hex')
        stringtopic = topic.toString('hex')
    }

    // FRESH FEEDS
    if (fresh) {
        cache['fresh'][next] = { sockets: {}, topics: {}, tasks: {} }
        // 1. Get or make feed 
        if (feedkey) feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
        else feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
        await ready(feed)
        if (!feedkey) {
            feedkey = feed.key
            topic = datdot_crypto.get_discoverykey(feedkey)
            stringtopic = topic.toString('hex')
            stringkey = feedkey.toString('hex')
        }
        log({ type: 'feed-store', data: { text: 'New feed - fresh', stringkey, stringtopic }})
        // 2. Store topic and feed
        cache['fresh'][next].topics = { [stringtopic]: { feeds: {}, sockets: {} } } // add next entry to cache
        cache['fresh'][next].topics[stringtopic].feeds[stringkey] = { feed }
        log({ type: 'feed-store', data: { text: 'New topic stored in cache - fresh', stringtopic }})
        // 3. Make swarm
        swarm = new hyperswarm()
        cache['fresh'][next].swarm = swarm
        log({ type: 'feed-store', data: { text: 'new swarm - fresh', mode }})
        swarm.on('connection', onconnection(cache, config, log))
        // 4. Join topic (if no discovery yet)
        var discovery = cache['fresh'][next].topics[stringtopic].discovery
        if (!discovery) discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        var tasks = cache['fresh'][next].tasks
        if (!tasks[stringtopic]) tasks[stringtopic] = 1
        else tasks[stringtopic]++
    } 
    // INTERCEPTED FEEDS
    else if (intercept) { 
        cache['intercept'] = { sockets: {}, topics: {}, tasks: {} }
        // 1. Get or make feed
        const topics = cache['intercept'].topics
        if (topics[stringtopic] && topics[stringtopic].feeds[stringkey]) { // get existing feed
            feed = topics[stringtopic].feeds[stringkey].feed 
            log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
        } else { // make new feed
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            await ready(feed)
            log({ type: 'feed-store', data: { text: 'New feed', intercept, stringkey, stringtopic }})
        }
        // 2. Store topic and feed
        if (!topics[stringtopic]) topics[stringtopic] = { feeds: {}, sockets: {} }
        if (!topics[stringtopic].feeds[stringkey]) { // stringtopic exists, let's add the stringkey to the feeds
            const db = sub(account.db, stringkey, { valueEncoding: 'binary' })
            const storage = new FeedStorage({ db, feed, log })
            topics[stringtopic].feeds[stringkey] = { feed, storage }
            log({ type: 'feed-store', data: { text: 'New feed and storage added in cache', intercept, stringkey, stringtopic }})
        }
        // 3. Make swarm
        if (!cache['intercept'].swarm) {
            const opts = { keyPair: { publicKey: account.noisePublicKey, secretKey: account.noisePrivateKey } }
            swarm = new hyperswarm(opts)
            cache['intercept'].swarm = swarm
            log({ type: 'feed-store', data: { text: 'new swarm - intercept', mode }})
            swarm.on('connection', onconnection(cache, config, log))
        } else swarm = cache['intercept'].swarm
        // 4. Join topic (if no discovery yet)
        var discovery = cache['intercept'].topics[stringtopic].discovery
        if (!discovery) discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        var tasks = cache['intercept'].tasks
        if (!tasks[stringtopic]) tasks[stringtopic] = 1
        else tasks[stringtopic]++    
        // 6. Add ext (if name & no ext yet)
        var ext = topics[stringtopic].ext
        if (!ext) topics[stringtopic].ext = register_extension(feed, name, ext_cbs.onmessage, ext_cbs.onerror)
    } 
    // REUSABLE UNINTERCEPTED FEEDS
    else { 
        cache['general'] = { sockets: {}, topics: {}, tasks: {} }
        const topics = cache['general'].topics
        // 1. Get or make feed
        if (topics[stringtopic] && topics[stringtopic].feeds[stringkey]) { // get existing feed
            feed = topics[stringtopic].feeds[stringkey].feed 
            log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
        } else { // make new feed
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            await ready(feed)
            log({ type: 'feed-store', data: { text: 'New feed', stringkey, stringtopic }})
        }
        // 2. Store topic and feed
        if (!topics[stringtopic]) topics[stringtopic] = { feeds: {}, sockets: {} }
        if (!topics[stringtopic].feeds[stringkey]) topics[stringtopic].feeds[stringkey] = { feed }
        // 3. Make swarm
        if (!cache['general'].swarm) {
            swarm = new hyperswarm()
            cache['general'].swarm = swarm
            log({ type: 'feed-store', data: { text: 'new swarm - general', mode }})
            swarm.on('connection', onconnection(cache, config, log))
        } else swarm = cache['general'].swarm
        // 4. Join topic (if no discovery yet)
        var discovery = cache['general'].topics[stringtopic].discovery
        if (!discovery) discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
        // 5. Add task
        var tasks = cache['general'].tasks
        if (!tasks[stringtopic]) tasks[stringtopic] = 1
        else tasks[stringtopic]++

    }
 
    log({ type: 'feed-store', data: { text: 'returning feed' }})
    return { feed, next }

    function onconnection (cache, config, log) {
        const { intercept, fresh } = config
        var cache_type
        var replicationStream

        if (fresh) cache_type = cache['fresh'][next]
        else if (intercept) cache_type = cache['intercept']
        else cache_type = cache['general']
        const { swarm, sockets, topics } = cache_type 

        return (connection, info) => {
            const remotekey = connection.remotePublicKey
            const remotestringkey = remotekey.toString('hex')
            if (sockets[remotestringkey]) {
                log({ type: 'load-feed', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = sockets[remotestringkey].replicationStream
            } else {
                log({ type: 'load-feed', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = new HypercoreProtocol(connection.isInitiator)
                sockets[remotestringkey] = { socket: connection, replicationStream }
                // connection.pipe(replicationStream).pipe(connection)
                pump(connection, replicationStream, connection, (error) => {
                    log({ type: 'load-feed', data: { text: 'Pumping finished' }})
                    if (error) console.log({ name: log.path, pumping_error: error})
                })
            }
            
            if (peerList.length) return is_on_peerList()
            
            // log({ type: 'swarm', data: { text: `New connection`, socket: socket.isInitiator  } })
            // next({ ext, feed, remotekey, log })

            /*
            1. hoster, downloading data from author
            2. feed is stored in cache and storage
            3. 
            */
            if (info.topics.length) {
                for (var i = 0, len = info.topics.length; i < len; i++) {
                    const Topic = info.topics[i].toString('hex')
                    log({ type: 'load-feed', data: { text: 'topics', Topic, isInitiator: connection.isInitiator } })
                    if (topics[Topic]) {
                        var topics_sockets = topics[Topic].sockets
                        if (!topics_sockets[[remotestringkey]]) topics_sockets[remotestringkey] = sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - client', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                }
            } else {
                replicationStream.on('discovery-key', (discoverykey) => { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
                    log({ type: 'onconnection', data: { text: 'discovery key' } })
                    const Topic = discoverykey.toString('hex')
                    if (topics[Topic]) { 
                        var topics_sockets = topics[Topic].sockets
                        if (!topics_sockets[[remotestringkey]]) topics_sockets[remotestringkey] = sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - server', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                })
            }
            function is_on_peerList () {
                var peerList_temp = [...peerList]
                log({ type: 'feed-store', data: { text: `is on peer list`, peerList  } })
                for (var i = 0, len = peerList_temp.length; i < len; i++) {
                    const hosterkey = peerList_temp[i]
                    log({ type: 'attestor challenge', data: { text: `Comparing keys`, peer: remotekey.toString('hex'), hoster: hosterkey.toString('hex') } })
                    if (remotekey.equals(hosterkey)) {
                        log({ type: 'feed-store', data: { text: `Found the right hoster`, id: hosterkey.toString('hex'), feed: feed.key.toString('hex')  } })
                        peerList_temp.splice(i, 1)
    
                        feed.replicate(replicationStream)
                        return onpeer(hosterkey)
                    }
                }
            }

            replicationStream.on('close', (err) => { log({ type: 'feed-store', data: { text: `replicationStream closed` } }) })
            replicationStream.on('end', (err) => { log({ type: 'feed-store', data: { text: `replicationStream ended` } }) })
            replicationStream.on('timeout', (err) => { log({ type: 'feed-store', data: { text: `replicationStream timed out` } }) })
        }
    }
}

async function _no_tasks (account, args) {
    const { stringtopic, log } = args
    delete account.cache.tasks[stringtopic]
    const remotekeys = Object.keys(account.cache.topics[stringtopic].sockets)
    // close all the streams
    remotekeys.forEach(key => {
        const { socket, replicationStream } = account.cache.sockets[key]
        // // if socket not in use, close it
        // socket.close()
        // replicationStream.close()
        // // delete the socket object
        // delete account.cache.sockets[key]
    })
    // close all the feeds
    const feedkeys = Object.keys(account.cache.topics[stringtopic].feeds)
    feedkeys.forEach(async key => {
        const feed = account.cache.topics[stringtopic].feeds[key]
        await feed.close()
    })
    // destroy the discovery
    // account.cache.topics[stringtopic].discovery.leave()
    await account.cache.topics[stringtopic].discovery.destroy()
    // delete the topic object
    delete account.cache.topics[stringtopic]
}


////////////////////////////////////////



////////////////////////////////////////

/* 
account = {
    cache:  {
        swarm,
        sockets: {
            remotekey: { 
                socket, 
                replicationStream
            }
        },
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { feed }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        },
        tasks: {
            stringkey: counter
        }
    },
    storage: {
        feedkey: { feed, db, discovery, unintercept }
    }
} 







 SCENARIOS

 cache: {
     'general': {
        swarm, // autogenerated keypair
        sockets,
        topics,
        tasks
     },
     'fresh': {
         '1': {
            swarm,
            sockets,
            topics,
            tasks    
        }
    },
     'intercept': {
        swarm, // noise keypair, stored on chain
        sockets,
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { 
                        feed,
                        storage
                    }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        },
        tasks
     }
 }

 ----
 ENCODING

 - starting

* feed !fresh
* can reuse a feed from other !fresh (sponsor)
* if no feed yet, make new
* if no swarm, create one (store it in cache under swarm pubkey)

 - finishing

* when feed not in use, close
* when swarm not in use, close
* when topic not in use, leave


 ----
 ATTESTING - performance challenge

  - starting

 * feed is fresh

 - finishing


 ----
 HOSTING

 - starting

 * feed is intercepted && !fresh


 - finishing

 ----
 AUTHOR

  - starting

* feed is fresh


 - finishing


 ----
 SPONSOR

 - starting

 * feed !fresh

 - finishing

---

*/

