const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const RAM = require('random-access-memory')
const datdot_crypto = require('../datdot-crypto')
const mux_proto = require('./mux-proto')
const b4a = require('b4a')
const c = require('compact-encoding')
const update_swarm_discovery = require('./update-swarm-discovery')


module.exports = feed_store

function feed_store (account, bootstrap) {
	const api = {
		load_feed: (opts) => _load_feed(account, opts),
		connect: (opts) => _connect(account, bootstrap, opts),
	}
	return api
}

function noop () {}

/* --------------------------------------------------------------------------------------------

																				LOAD FEED

-----------------------------------------------------------------------------------------------*/
async function _load_feed(account, { newfeed = true, feedkey, topic, log }) { // newfeed false for 
	log({ type: 'feed-store', data: { text: 'loading feed', feedkey, topic }})
	var feed
	var stringkey
	var stringtopic
	if (!account.cache) account.cache = { sockets: {}, topics: {}, feeds: {}, peers: {}, tasks: {} }
	const feeds = account.cache.feeds
	
	// 0. If feedkey and/or topic, get stringkey and stringtopic
	if (topic) { // when msg.receive or msg.send (no feedkey, only topic)
		topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
		stringtopic = topic.toString('hex') 
	}
	if (feedkey) { // when feed key with or without topic
		feedkey = Buffer.isBuffer(feedkey) ? feedkey : b4a.from(feedkey, 'hex')
		stringkey = feedkey.toString('hex')
		if (!topic) {
			topic = datdot_crypto.get_discoverykey(feedkey)
			stringtopic = topic.toString('hex')
		}
	} 
	
	// 1. Get or make feed
	if (stringtopic && stringkey && feeds[stringtopic]) { // get existing feed
		log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic, feeds: Object.keys(feeds[stringtopic]) }})
		feed = feeds[stringtopic][stringkey].feed 
	} else { // make new feed
		if (newfeed) { 
			if (feedkey) {
				log({ type: 'feed-store', data: { text: 'Making new feed with feedkey', stringkey, stringtopic }})
				feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
				await feed.ready()
			} else {
				feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
				log({ type: 'feed-store', data: { text: 'Making new feed without feedkey', stringkey, stringtopic }})
				await feed.ready()
				feedkey = feed.key
				stringkey = feedkey.toString('hex')
				if (!topic) topic = datdot_crypto.get_discoverykey(feedkey)
				if (!stringtopic) stringtopic = topic.toString('hex')
			}
			log({ type: 'feed-store', data: { text: 'New feed', stringkey, stringtopic }})
		}
		// 2. Store topic and new feed (if available)
		feeds[stringtopic] = {}  // add topic to the feeds
		if (feed) feeds[stringtopic] = { [stringkey]: { feed } }
		log({ type: 'feed-store', data: { text: 'New feed stored in cache', stringtopic }})
	}

	log({ type: 'feed-store', data: { text: 'returning feed', stringkey, returning_stringtopic: stringtopic }})
	return { feed }
	
}

/* --------------------------------------------------------------------------------------------

																				CONNECT

-----------------------------------------------------------------------------------------------*/
async function _connect ( account, bootstrap, { swarm_opts, peers, log }) {
	const { noisePublicKey, noisePrivateKey } = account
	
	peers = Object.assign({ peerList: [], onpeer: noop, msg: {} }, peers)
	const { peerList, onpeer, msg, feed } = peers
	var { role, topic, mode } = swarm_opts
	var stringtopic
	var discovery
	
	log({ type: 'connect', data: { text: 'load-store connect', msg }})

	// topic created with derive-topic is not a valid topic
	// but we connect to peers on peerList directly so the topic
	// is this case is just used to store sockets etc. (topics[stringtopic])
	if (topic) 
	topic = Buffer.isBuffer(topic) ? topic : b4a.from(topic, 'hex')
	stringtopic = topic.toString('hex') 
	if (!topic) throw new Error('Can not connect without topic')
	
	var { topics, swarm } = account.cache

	// 2. Make swarm
	if (!swarm) {
		swarm = new hyperswarm({ 
			bootstrap, 
			keyPair: { publicKey: noisePublicKey, secretKey: noisePrivateKey }, 
			maxParallel: Infinity, 
			maxPeers: Infinity 
		})
		account.cache.swarm = swarm
		log({ type: 'store connect', data: { text: 'new swarm', mode }})
		swarm.on('connection', onconnection(account, log))
	}

	// 3. Store topic and feed (for new feed)
	if (!topics[stringtopic]) {
		topics[stringtopic] = { // add topic to the cache
			sockets: {}, 
			tasks: { [`${role}`]: 1 } 
		} 
		log({ type: 'feed-store', data: { text: 'New topic in cache', stringtopic, tasks: topics[stringtopic].tasks }})
	} else {
		const tasks = topics[stringtopic].tasks
		tasks[`${role}`] ? tasks[`${role}`]++ : tasks[`${role}`] = 1
		log({ type: 'feed-store', data: { text: 'Existing topic in cache', stringtopic, tasks }})
	}

	// 3. Join peer through existing channel or join the swarm on topic
	var discovery = swarm.status(topic)

	if (peerList.length === 0) { // Join topic (if no peerList && no discovery yet)
		if (!discovery) {
			log({ type: 'store connect', data: { text: 'making new discovery' }})
			await swarm.join(topic, mode).flushed()
			topics[stringtopic].discovery = swarm.status(topic)
			log({ type: 'store connect', data: { text: 'joined the swarm', mode, stringtopic }})
		} else {
			mode =  await update_swarm_discovery({ swarm, topic, tasks: topics[stringtopic].tasks, log })
			log({ type: 'store connect', data: { text: 'updated swarm discovery', mode, stringtopic }})
		}
	} else { // connect only to the peers on the list
		log({ type: 'store connect', data: { text: 'connecting only to peers', peerList: peerList.length }})
		for (var i = 0; i < peerList.length; i++) {
			const remotestringkey = peerList[i]
			const cache = account.cache
			const peers = cache.peers
			if (!peers[remotestringkey]) peers[remotestringkey] = { topics: { [stringtopic]: { onpeer, feed, msg, log } } }
			else peers[remotestringkey].topics[stringtopic] = { onpeer, feed, msg, log }
						
			const sockets = cache.sockets
			
			if (sockets[remotestringkey]) {
				log({ type: 'store connect', data: { text: 'existing socket' }})
				console.log({text: 'existing socket :)', remotestringkey, sock: sockets[remotestringkey] })
				exchange_feedkey({ account, remotestringkey, log }) // feed will be returned with onpeer callback
				sockets[remotestringkey].count++
				continue
			} else {				
				// Join peer (if no socket connection && no discovery yet)
				log({ type: 'store connect', data: { text: 'joining peer in the swarm directly', remotestringkey, stringtopic, mode }})
				swarm.joinPeer(b4a.from(remotestringkey, 'hex'))
				await swarm.listen() // explicitly start listening for incoming connections
			}
		}
	}
	
}





/* --------------------------------------------------------------------------------------------

																				ONCONNECTION

-----------------------------------------------------------------------------------------------*/

function onconnection (account, conn_log) {
	return async (connection, peerInfo) => {
		try {
			const { cache } = account
			const { swarm, topics, peers, sockets } = cache
			const remotekey = peerInfo.publicKey
			const remotestringkey = remotekey.toString('hex')	
			const conn_log = account.log.sub(`<-onconnection: me: ${account.noisePublicKey.toString('hex').substring(0,5)}, peer: ${remotestringkey.substring(0,5)} `)
			conn_log({ type: 'onconnection', data: { text: 'connection callback', remotestringkey } })
			
			const target = peers[remotestringkey]

			const { replicationStream } = await load_replicationStream_and_channel({ account, connection, remotestringkey, log: conn_log })
			
			// // make sure you only connect once to the same peer, but keep the count
			
			// see if this peer is on the list of peers you need to connect to for a specific task
			if (target) {
				conn_log({ type: 'onconnection', data: { text: 'target exists', remotestringkey } })
				return exchange_feedkey({ account, remotestringkey, log: conn_log }) // this is the peer we need to exchange feedkeys with
			}
			// handle client (isInitiator is true)
			if (peerInfo.topics.length) {
				for (var i = 0, len = peerInfo.topics.length; i < len; i++) {
					const topic = peerInfo.topics[i]
					const stringtopic = topic.toString('hex') // only shows topics derived from a feed.discoveryKey
					conn_log({ type: 'onconnection - client', data: { text: 'looping over topics', count: peerInfo.topics.length, i, stringtopic } })
					if (topics[stringtopic]) {
						conn_log({ type: 'onconnection - client', data: { text: 'matching topic', remotestringkey, isInitiator: connection.isInitiator, stringtopic } })
						// if (topics[stringtopic].sockets[remotestringkey]) return
						set_topics_socket({ topics, stringtopic, key: remotestringkey, log: conn_log })
						replicate_feeds({ cache, stringtopic, replicationStream, log: conn_log })
					}
				}
			} 
		} catch (err) {
			conn_log({ type: 'Error', data: { text: 'Error: in onconnection', err }})
		}
	}
}		
/*-----------------------------------------------------------------------------------------------------
		
																										HELPERS
		
		------------------------------------------------------------------------------------------------------*/

	function set_topics_socket ({ topics, stringtopic, key, log}) {
		if (!topics[stringtopic].sockets[[key]]) {
			log({ type: 'onconnection', data: { text: 'setting topics socket key', stringtopic, key } })
			topics[stringtopic].sockets[key] = true
		}
	}
	
	async function load_replicationStream_and_channel ({ account, connection, remotestringkey, log }) {
		try {
			const sockets = account.cache.sockets
			const topics = account.cache.topics
			const target = account.cache.peers[remotestringkey]
			var replicationStream
			var mux
	
			
			if (sockets[remotestringkey]) {
				log({ type: 'onconnection', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
				sockets[remotestringkey].count++
				replicationStream = sockets[remotestringkey].replicationStream
				mux = sockets[remotestringkey].mux
				
			} else {
				log({ type: 'onconnection', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
				replicationStream = hypercore.createProtocolStream(connection, { ondiscoverykey })
				
				replicationStream.on('error', (err) => { log({ type: 'onconnection', data: { text: `Error: replicationStream error`, err } }) })
				replicationStream.on('close', (err) => { log({ type: 'onconnection', data: { text: `onclose: replicationStream closed` } }) })
				replicationStream.on('end', (err) => { log({ type: 'onconnection', data: { text: `onend: replicationStream ended` } }) })
				replicationStream.on('timeout', (err) => { log({ type: 'onconnection', data: { text: `ontimeout: replicationStream timed out` } }) })
				
				mux = hypercore.getProtocolMuxer(replicationStream)
				sockets[remotestringkey] = { socket: connection, replicationStream, mux, count: 1 }
				await new_channel({ account, remotestringkey, log })
			}
	
			return { replicationStream }

			// handle server (isInitiator is false)
			function ondiscoverykey (discoverykey) { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
				log({ type: 'ondiscovery', data: { text: 'on discovery key' } })
				if (target) return // if peer from the peerList, don't do anything
				const stringtopic = discoverykey.toString('hex') // only shows topics derived from a feed.discoveryKey
				// see if there is task for this topic and that we are not yet connected to the peer for this topic
				if (topics[stringtopic]) {
					log({ type: 'ondiscovery', data: { text: 'matching topic', remotestringkey, isInitiator: connection.isInitiator, stringtopic } })
					set_topics_socket({ topics, stringtopic, key: remotestringkey, sockets, log })
					replicate_feeds({ cache: account.cache, stringtopic, replicationStream, log })
				}
			}
		} catch (err) {
			log({ type: 'Error', data: { text: 'Error: in loading replication stream and channel', err }})
		}
		
	}
	
	function replicate_feeds ({ cache, stringtopic, replicationStream, log }) {
		try {
			const { swarm, feeds, topics }  = cache
			const topic = b4a.from(stringtopic, 'hex')

			// if (!topics[stringtopic].count) return 

			const status = swarm.status(topic)
			if (!status) return
			
			log({ type: 'replicate feeds', data: { text: 'feeds for topic', stringtopic, count: Object.keys(feeds[stringtopic]).length } })
			
			const keys = Object.keys(feeds[stringtopic])
			const len = keys.length
			for (var i = 0; i < len; i++) {
				const key = keys[i]
				const feed = feeds[stringtopic][key].feed
				log({ type: 'replicate feeds', data: { text: 'replicating feed', stringtopic, feedkey: feed.key.toString('hex') } })
				if (!feed.opened || feed.closing) continue
				feed.replicate(replicationStream)
			}
		} catch (err) {
			log({ type: 'Error', data: { text: 'Error: replicating feeds in the general swarm', err }})
		}
	}

	async function exchange_feedkey ({ account, remotestringkey, log }) { //get feedkey from or send to a target peer (on each custom topic)
		log({ type: 'exchange feedkey', data: { text: `is on peerList`, remotestringkey  } })				
		const { cache } = account
		const {peers: Peers, sockets, topics, feeds, swarm } = cache
		const peer_topics = Object.keys(Peers[remotestringkey].topics)
		const { replicationStream, channel } = sockets[remotestringkey]

		// TODO: optimize to create channel only for exchange feedkey peers
		// make sure you have the code that doesn't need to open the channel in the same tick
		// await new_channel({ account, remotestringkey, log })

		// get string message type for sending strings through the mux channel with that peer (channel.addMessage(...))
		const string_msg = sockets[remotestringkey].channel.messages[0]
		
		// load registry
		var registry = string_msg.registry

		// we need to loop over all the topics for this peer we just got connected to 
		// in order to find all the topics we have to connect for (peerInfo.topics only shows
		// topics derived from a feedkey, not custom topics
		const len = peer_topics.length

		for (var i = 0; i < len; i++) {
			const stringtopic = peer_topics[i]
			// see if we are already connected to this peer for this topic task
			// if (topics[stringtopic].sockets[remotestringkey]) continue 

			// set_topics_socket({ topics, stringtopic, key: remotestringkey, log })

			const { msg, log } = Peers[remotestringkey].topics[stringtopic]
			log({ type: 'exchange feedkey', data: { text: `is on for peer and topic`, remotestringkey, peer_topics, stringtopic } })				
			if (!msg) continue // we already exchanged key for this topic
			log({ type: 'exchange feedkey', data: { text: `current peer ${remotestringkey} for topic ${stringtopic}`, peer_topics  } })

			// get entry for stringtopic
			const entry = registry.get(stringtopic)				
			if (entry) {
				const { msg_queue } = entry
				if (msg_queue) msg_queue.forEach(onmessage)
				entry.msg_queue = void 0
				entry.onmessage = onmessage
			} else {
				string_msg.registry.set(stringtopic, { onmessage })
			}		
			try {
				if (msg.send) { // see if for this topic we send or receive a message
					const { feed } = Peers[remotestringkey].topics[stringtopic]
					log({ type: 'exchange feedkey', data: { text: `sending the message` } })
					string_msg.send(JSON.stringify({ type: 'feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
					log({ type: 'exchange feedkey', data: { text: 'msg sent - feedkey', feedkey: feed.key.toString('hex'), stringtopic }})
				} else if (msg.receive) {
					log({ type: 'exchange feedkey', data: { text: `receiving the message` } })
				}
			} catch (err) {
				log({ type: 'error', data: { text: 'Error: is on peerList', err } })
			}
			
		}

		async function onmessage (message) {
			console.log({ type: 'onmessage protomux', message })
			const { type, feedkey, stringtopic } = JSON.parse(message)
			const { onpeer, msg, log } = Peers[remotestringkey].topics[stringtopic]
			var feed
			if (type === 'feedkey') {	
				// replicationStream.write((JSON.stringify({ type: 'feedkey-ack' })))
				// 1. Make feed
				log({ type: 'onmessage protomux', data: { text: 'msg received - feedkey', feedkey: feedkey.toString('hex') }})
				feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
				await feed.ready()
				// feedkey = feed.key
				stringkey = feedkey.toString('hex')
				feeds[stringtopic][stringkey] = { feed }
				log({ type: 'onmessage protomux', data: { text: 'New feed - exchange key', stringkey }})
				string_msg.send(JSON.stringify({ type: 'ack-feedkey', feedkey: feed.key.toString('hex'), stringtopic }))
				replicate_to_peer({ cache, onpeer, feed, replicationStream, stringtopic, log })
				string_msg.registry.delete(stringtopic)				
			} 
			else if (type === 'ack-feedkey') {
				feed = Peers[remotestringkey].topics[stringtopic].feed
				const stringkey = feed.key.toString('hex')
				log({ type: 'onmessage protomux', data: { text: 'Ack received', stringkey }})
				replicate_to_peer({ cache, onpeer, feed, replicationStream, stringtopic, log })
				string_msg.registry.delete(stringtopic) // all messages for this topic have been sent/received
				// sockets[remotestringkey].channel.close()
			}

			function replicate_to_peer ({ onpeer, feed, replicationStream, stringtopic, log }) {
				const Peers = cache.peers
				log({ type: 'replicate feed', data: { text: 'Replicate to peer' }})
				if (!feed.opened || feed.closing) return
				feed.replicate(replicationStream)
				delete Peers[remotestringkey].topics[stringtopic]
				if (peer_topics.length === 0) delete Peers[remotestringkey]
				onpeer({ feed, remotekey: remotestringkey })
			}
		}		
	}

	function new_channel ({ account, remotestringkey, log }) {
		return new Promise(async (resolve, reject) => {
			const sockets = account.cache.sockets
			const { mux, channel } = sockets[remotestringkey]
			log({ type: 'open protomux channel', data: { text: `new channel`, remotestringkey } })
			// TODO: we need to enable creation of new channel not in the same tick
			if (channel) return resolve()
			
			const make_unique = () => {
				const mystringkey = account.noisePublicKey.toString('hex')
				var arr = [mystringkey, remotestringkey]
				arr = arr.sort((a,b) => a < b ? -1 : 1)
				return arr.reduce((a, b) => a + b, 0)
			}
			const opts = { protocol: make_unique() }
			await mux_proto.make_protocol(mux, opts, cb)
	
			function onopen () { log({ type: 'open protomux channel', data: { text: 'onopen: the other side opened this protocol!' }}) }
			function onclose () { log({ type: 'open protomux channel', data: { text: 'onclose: either side closed the protocol' }}) }
	
			async function cb () {
				log({ type: 'mux proto cb', data: { text: 'mux stream to peer opened' } })
				mux.unpair(opts)
		
				const messages = [ // add message types for different encodings (i.e. string...)
					{ encoding: c.string, onmessage: dispacher }
				]
				const channel = mux_proto.create_and_open_channel(mux, {...opts, onopen, onclose, messages })
				const string_msg = channel.messages[0]

				sockets[remotestringkey].channel = channel
								
				// load registry to store onmessage call backs and undelivered messages (msg_queue) for each stringtopic
				var registry = string_msg.registry
				if (!registry) registry = string_msg.registry = new Map()
				log({ type: 'open protomux channel', data: { text: `registry loaded` } })
				
				// intercept onmessage with dispacher that will call the right callback
				log({ type: 'open protomux channel', data: { text: `onmessage set to dispacher` } })
				
				resolve()

				function dispacher (message) {
					const { stringtopic } = JSON.parse(message)
					const reg = registry.get(stringtopic)
					if (reg) {
						log({ type: 'dispacher', data: { text: 'reg exists', reg } })
						const { onmessage, msg_queue } = reg
						onmessage ? onmessage(message) : msg_queue.push(message)
					} else {
						registry.set(stringtopic, { msg_queue: [message] })
					}
					
					log({ type: 'open protomux channel', data: { text: `dispacher gets onmessage`, message, registry: [...registry.entries()] } })
				}
			}	
		})
	}



/* --------------------------------------------------------------------------------------------

																				CACHE

-----------------------------------------------------------------------------------------------*/

/* 
account = {
    cache:  {
        swarm,
        sockets: {
            [remotestringkey]: { 
								count: 1 // incremented for every new task for that peer
                socket, 
                replicationStream,
								mux,
								channel: {
									messages: {
										// different types, our only type for now is string_msg (channel.messages[0])
										registry: { msg_queue: [message], onmessage }
										onmessage: dispacher
									}
								},
            }
        },
        topics: {
            [stringtopic]: {
							tasks: {
										author: 0, // { server: true }
										sponsor: 0, // { client: true } 
										encoder2author: 1, // { client: true }
										encoder2attestor: 0, // { server: true }
										attestor2encoder: 0, // { client: true }
										attestor2hoster: 0, // { server: true }
										hoster2author: 0, // { server: true, client: true }
										hoster2attestor: 0, // { client: true }
										hoster2peers: 0, // { server: true }
										perf_attestor: 0 
							},
							sockets: {
								[remotekey]: true
							}
              discovery,
            }
        },
				feeds: {
					[stringtopic]: {
						[stringkey]: { feed }
					}
				}
				// peers are created only when we have a msg (send or receive) and a peerList and we need to exchange keys first and then replicate
				peers: {  
					[remotestringkey]: {
						topics: {
							[stringtopic1]: { feed, msg: { receive, send }, onpeer, log },
							[stringtopic2]: { feed, msg: { receive, send }, onpeer, log }
						}
					}
				},
    },
    storage: {
        feedkey: { feed, db, discovery, unintercept }
    }
} 

*/

