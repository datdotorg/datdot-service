const hyperswarm = require('hyperswarm')
const hypercore = require('hypercore')
const HypercoreProtocol = require('hypercore-protocol')
const RAM = require('random-access-memory')
const register_extension = require('_datdot-service-helpers/register-extension')
const ready = require('_datdot-service-helpers/hypercore-ready')
const datdot_crypto = require('../datdot-crypto')
const FeedStorage = require('_datdot-service-helpers/feed-storage.js')
const sub = require('subleveldown')
var pump = require('pump')

module.exports = feed_store

function feed_store (account) {
    const api = {
        load_feed: (opts) => _load_feed(account, opts),
        no_tasks: (...args) => _no_tasks(account, ...args)
    }
    return api
}

function noop () {}

async function _load_feed(account, { config, extension = {}, swarm_opts, peers, feedkey, log }) {
    const { intercept, fresh, persist } = config // feed config
    var { topic, mode } =  swarm_opts
    const { ext_cbs, name } = extension
    peers = Object.assign({ peerList: [], onpeer: noop }, peers)
    const { peerList, onpeer } = peers
    
    log({ type: 'feed-store', data: { text: 'loading feed', config, extension, swarm_opts, peers, feedkey }})
    
    var feed
    var feedkey
    var stringkey
    var swarm
    var stringtopic
    var storage
    const cache = account.cache

    if (feedkey) {
        topic = datdot_crypto.get_discoverykey(feedkey)
        stringkey = feedkey.toString('hex')
        stringtopic = topic.toString('hex')
    }
    
    // 1. Get or make feed
    
    // make fresh feed (if fresh or if hoster with no storage for this key)
    if (fresh) {
        feed = new hypercore(RAM, { valueEncoding: 'binary', sparse: true })
        await ready(feed)
        feedkey = feed.key
        topic = datdot_crypto.get_discoverykey(feedkey)
        stringtopic = topic.toString('hex')
        stringkey = feedkey.toString('hex')
        log({ type: 'feed-store', data: { text: 'New feed - fresh', stringkey, stringtopic }})
    } 
    else if (intercept) {
        if (!account.storages.has(stringkey)) { // make  new feed as a hoster
            feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
            await ready(feed)
            log({ type: 'feed-store', data: { text: 'New feed - intercept', stringkey, stringtopic }})
            // add feed to storage if no storage for this feed & hoster (intercept is true)
            const db = sub(account.db, stringkey, { valueEncoding: 'binary' })
            const storage = new FeedStorage({ db, feed, log })
            account.storages.set(stringkey, storage)       
        } else if (account.storages.has(stringkey)) { // get existing feed (if storage exists)
            storage = await account.storages.get(stringkey)
            log({ type: 'feed-store', data: { text: 'Existing feed - intercept', stringkey, stringtopic }})
            feed = storage.feed
        }
    } else if (cache.topics[stringtopic] && cache.topics[stringtopic].feeds[stringkey]) { // get existing feed
        feed = cache.topics[stringtopic].feeds[stringkey].feed 
        log({ type: 'feed-store', data: { text: 'Existing feed', stringkey, stringtopic }})
    } else { // make new feed
        feed = new hypercore(RAM, feedkey, { valueEncoding: 'binary', sparse: true })
        await ready(feed)
        log({ type: 'feed-store', data: { text: 'New feed', stringkey, stringtopic }})
    }
    
    // 2. Store topic and feed
    if (!cache.topics[stringtopic]) {
        cache.topics = { [stringtopic]: { feeds: {}, sockets: {} } }
        log({ type: 'feed-store', data: { text: 'New topic stored in cache', stringtopic }})
    }
    if (!cache.topics[stringtopic].feeds[stringkey]) {
        cache.topics[stringtopic].feeds[stringkey] = { feed }
        log({ type: 'feed-store', data: { text: 'New feed stored in cache', stringkey, stringtopic }})
    }
    // 3. Make swarm
    const opts = { keyPair: { publicKey: account.noisePublicKey, secretKey: account.noisePrivateKey } }
    if (!cache.swarm) {
        swarm = new hyperswarm(opts)
        cache.swarm = swarm
        log({ type: 'feed-store', data: { text: 'new swarm', mode }})
        swarm.on('connection', onconnection(cache, log))
    } else swarm = cache.swarm
    
    // 4. Join topic (if no discovery yet)
    const discovery = cache.topics[stringtopic].discovery
    if (!discovery) {
        cache.topics[stringtopic].discovery = swarm.join(topic, mode)
        log({ type: 'feed-store', data: { text: 'swarm joined', mode }})
    }
    if (storage) storage.discovery = cache.topics[stringtopic].discovery
    
    // 5. Add task
    var tasks = cache.tasks[stringtopic]
    if (!tasks) cache.tasks[stringtopic] = 1
    else tasks++
        
    // 6. Add ext (if name & no ext yet)
    if (name && !cache.topics[stringtopic].ext) {
        cache.topics[stringtopic].ext = register_extension(feed, name, ext_cbs.onmessage, ext_cbs.onerror)
        log({ type: 'feed-store', data: { text: 'extension registered', name }})
    }

    log({ type: 'feed-store', data: { text: 'returning feed' }})
    console.log({ name: log.path, feed_returned: feed })
    return feed

    function onconnection (cache, log) {
        const { swarm, sockets, topics } = cache
        var replicationStream
        return (connection, info) => {
            const remotekey = connection.remotePublicKey
            const remotestringkey = remotekey.toString('hex')
            if (sockets[remotestringkey]) {
                log({ type: 'load-feed', data: { text: 'existing connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = sockets[remotestringkey].replicationStream
            } else {
                log({ type: 'load-feed', data: { text: 'new connection', remotestringkey, isInitiator: connection.isInitiator } })
                replicationStream = new HypercoreProtocol(connection.isInitiator)
                cache.sockets[remotestringkey] = { socket: connection, replicationStream }
                // connection.pipe(replicationStream).pipe(connection)
                pump(connection, replicationStream, connection, (error) => {
                    log({ type: 'load-feed', data: { text: 'Pumping finished' }})
                    if (error) console.log({ name: log.path, pumping_error: error})
                })
            }
            
            if (peerList.length) return is_on_peerList()
            
            // log({ type: 'swarm', data: { text: `New connection`, socket: socket.isInitiator  } })
            // next({ ext, feed, remotekey, log })

            /*
            1. hoster, downloading data from author
            2. feed is stored in cache and storage
            3. 
            */
            if (info.topics.length) {
                log({ type: 'load-feed', data: { text: 'topics', topics: info.topics, isInitiator: connection.isInitiator } })
                for (var i = 0, len = info.topics.length; i < len; i++) {
                    const Topic = info.topics[i].toString('hex')
                    if (topics[Topic]) {
                        var topics_socket = topics[Topic].sockets[remotestringkey]
                        if (!topics_socket) topics[Topic].sockets[remotestringkey] = cache.sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - client', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                }
            } else {
                replicationStream.on('discovery-key', (discoverykey) => { // client hasn't replicated the hypercores so they are asking for the 'discovery-key' event
                    log({ type: 'onconnection', data: { text: 'discovery key' } })
                    const Topic = discoverykey.toString('hex')
                    if (topics[Topic]) { 
                        var topics_socket = topics[Topic].sockets[remotestringkey]
                        if (!topics_socket) topics[Topic].sockets[remotestringkey] = cache.sockets[remotestringkey]
                        Object.keys(topics[Topic].feeds).forEach(key => {
                            const feed = topics[Topic].feeds[key].feed
                            log({ type: 'onconnection', data: { text: 'matching topic - server', remotestringkey, isInitiator: connection.isInitiator, topic: Topic } })
                            feed.replicate(replicationStream)
                        }) 
                    }
                })
            }
            function is_on_peerList () {
                var peerList_temp = [...peerList]
                log({ type: 'feed-store', data: { text: `is on peer list`, peerList  } })
                for (var i = 0, len = peerList_temp.length; i < len; i++) {
                    const hosterkey = peerList_temp[i]
                    log({ type: 'attestor challenge', data: { text: `Comparing keys`, peer: remotekey.toString('hex'), hoster: hosterkey.toString('hex') } })
                    if (remotekey.equals(hosterkey)) {
                        log({ type: 'feed-store', data: { text: `Found the right hoster`, id: hosterkey.toString('hex'), feed: feed.key.toString('hex')  } })
                        peerList_temp.splice(i, 1)
    
                        feed.replicate(replicationStream)
                        return onpeer(hosterkey)
                    }
                }
            }

            replicationStream.on('close', (err) => { log({ type: 'feed-store', data: { text: `replicationStream closed` } }) })
            replicationStream.on('end', (err) => { log({ type: 'feed-store', data: { text: `replicationStream ended` } }) })
            replicationStream.on('timeout', (err) => { log({ type: 'feed-store', data: { text: `replicationStream timed out` } }) })
        }
    }
}

async function _no_tasks (account, args) {
    const { stringtopic, log } = args
    delete account.cache.tasks[stringtopic]
    const remotekeys = Object.keys(account.cache.topics[stringtopic].sockets)
    // close all the streams
    remotekeys.forEach(key => {
        const { socket, replicationStream } = account.cache.sockets[key]
        // // if socket not in use, close it
        // socket.close()
        // replicationStream.close()
        // // delete the socket object
        // delete account.cache.sockets[key]
    })
    // close all the feeds
    const feedkeys = Object.keys(account.cache.topics[stringtopic].feeds)
    feedkeys.forEach(async key => {
        const feed = account.cache.topics[stringtopic].feeds[key]
        await feed.close()
    })
    // destroy the discovery
    // account.cache.topics[stringtopic].discovery.leave()
    await account.cache.topics[stringtopic].discovery.destroy()
    // delete the topic object
    delete account.cache.topics[stringtopic]
}


////////////////////////////////////////



////////////////////////////////////////

/* 
account = {
    cache:  {
        swarm,
        sockets: {
            remotekey: { 
                socket, 
                replicationStream
            }
        },
        topics: {
            stringtopic: {
                discovery,
                feeds: {
                    stringkey: { feed }
                },
                sockets: {
                    remotekey: account.cache.sockets[remotekey]
                }
            }
        }
    },
    storage: {
        feedkey: { feed, db, discovery, unintercept }
    }
} 

*/