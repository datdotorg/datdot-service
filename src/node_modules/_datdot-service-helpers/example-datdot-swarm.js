// datdot-swarm = independent module
// datdot-service = uses datdot-swarm to do its job well
const datdot_chainjs = require('datdot-chain-javascript')
const chainjs = datdot_chainjs()

chainjs.on(event => {
	if (is_attest_hosting_setup(event)) return handle_attest_hosting_setup(event)
	if (is_host_hosting_setup(event)) return handle_host_hosting_setup(event)
	if (is_encode_hosting_setup(event)) return handle_encode_hosting_setup(event)
	if (is_performance_benchmark(event)) return handle_performance_benchmark(event)
	if (is_storage_verification_hosting(event)) return handle_storage_verification_hosting(event)
	if (is_storage_verification_attesting(event)) return handle_storage_verification_attesting(event)
})

const ram = require('random-access-memory')
const hypercore = require('hypercore')
const datdot_swarm = require('datdot-swarm')
const swarm_service = datdot_swarm() // create swarm peer (TODO: maybe separate process?)

// ----------------------------------------------------------------------------
// SCENARIO 1
// ----------------------------------------------------------------------------
async function handle_attest_hosting_setup (event) {
	const {
		event_id,
		amendmend_id: hosting_setup_id,
		attesterkey,
		encoderkeys,
		hosterkeys
	} = await chainjs.get_info_attest_hosting_setup(event)

	// TODO: external: join 6 connections (3 encoders, 3 hosters)
	encoderkeys.forEach((encoderkey, i) => {
		const hosterkey = hosterkeys[i]
		// ------------------------------------
		// connect to peer
		// ------------------------------------
		swarm_service.connect_to_peer(hosterkey, hoster_setup_id).then(async peer => {
			// ...
		})
		swarm_service.connect_to_peer(encoderkey, hoster_setup_id).then(async peer => {
			// TODO: replicate all 6 cores (3 enc, 3 hosters)
			// TODO: receive 3 feed.keys from encoders + create cores & send 3 keys to hosters
			// TODO: donwload chunks from enc + append chunks for hosters
			// (socket, info)
		})
	})
}
// ----------------------------------------------------------------------------
// SCENARIO 2
// ----------------------------------------------------------------------------
async function handle_host_hosting_setup (event) {
	const { event_id } = await get_info_host_hosting_setup(event)
	
	
	// join the swarm
	// replicate feed
	// dowload chunks
	// disconnect, and close streams BUT stay don't leave the topic
	

	// ------------------------------------
	// connect to peers on topic
	// ------------------------------------
	const peerkeys = ['peerkey1', 'peerkey2']
	const peers = swarm_service.connect_to_peers_on_topic({ topic, peerkeys })
	for (var i = peers.length; i--;) handle_peer(peers[i])

	function handle_peer (peer) {

	}

	// ------------------------------------
	// connect to peers on topic
	// ------------------------------------
	swarm_service.connect_to_peers_on_topic({ topic, peers: [] }).then(async topic => {

		const feed = new hypercore(ram(), feedkeyBuf, { valueEncoding: 'binary' }) // reader feed
		const stream = feed.replicate()
		const pipeline = [peer, stream, peer]

		topic.on('connection', peer => {
			connect(pipeline, function done () {

			})
		})
	})

	// ------------------------------------
	// connect to topic
	// ------------------------------------
	swarm_service.connect_to_topic({ topic, server, client }).then(async (topicstream) => {

		var feed = hypercore.replicate()
		const stream = feed.replicate()

		connect([stream, topicstream, stream], function onEnd () {

		})

		topicstream.leave()


	})


	// join 2 hyperbeams
	// get core key
	// disconnect, and close streams and leave the topics for once_stream
	// replicate core
	// download encoded data
	// disconnect, and close streams and leave the topics

}
// ----------------------------------------------------------------------------
// SCENARIO 3
// ----------------------------------------------------------------------------
async function handle_encode_hosting_setup (event) {
	const { event_id } = await get_info_encode_hosting_setup(event)
	
	// join the swarm
	// replicate feed
	// donwload chunks
	// disconnect, leave the topic and close streams

	// join 2 hyperbeams
	// make a core & send core key
	// disconnect, and close streams and leave the topics for once_stream
	// replicate core
	// append encoded data
	// disconnect, and close streams and leave the topics

}
// ----------------------------------------------------------------------------
// SCENARIO 4
// ----------------------------------------------------------------------------
async function handle_performance_benchmark (event) {
	const { event_id } = await get_info_performance_benchmark(event)
	
	// join the topic
	// loop
		// wait for hoster to connect
		// replicate feed
		// download chunks
		// disconnect and close streams
	// leave the topic (if no active challenges for same feed)

}
// ----------------------------------------------------------------------------
// SCENARIO 5
// ----------------------------------------------------------------------------
async function handle_storage_verification_attesting (event) {
	const { event_id } = await get_info_storage_verification_attesting(event)

	// create 2 hyperbeams *(a & b): topic a: pubkeys + event_id, topic b: pubkeys + event_id + once)
	// get key of new hypercore over beam b
	// disconnect, leave the topic and close streams for once_beam
	// replicate hypercore over beam a
	// download chunks
	// disconnect, leave the topic and close streams
	
}
// ----------------------------------------------------------------------------
// SCENARIO 6
// ----------------------------------------------------------------------------
async function handle_storage_verification_hosting (event) {
	const { event_id } = await get_info_storage_verification_hosting(event)
	
	// create 2 hyperbeams *(a & b): topic a: pubkeys + event_id, topic b: pubkeys + event_id + once)
	// make core & send key over beam b
	// disconnect, leave the topic and close streams for once_beam
	// replicate hypercore over beam a
	// append requested chunks to the hypercore
	// disconnect, leave the topic and close streams

}
// ----------------------------------------------------------------------------